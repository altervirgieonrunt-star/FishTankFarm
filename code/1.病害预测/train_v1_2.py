"""
ç—…å®³é¢„æµ‹ v1.2ï¼šPINN åäº‹å®åˆæˆ + è¿ç§»å­¦ä¹ å¾®è°ƒ
==============================================
åœ¨ v1.1ï¼ˆç‰©ç†ç‰¹å¾ + åŸŸè‡ªé€‚åº”ï¼‰åŸºç¡€ä¸Šæ–°å¢ä¸¤å¤§æ”¹è¿›ï¼š

  A. PINN åäº‹å®åˆæˆï¼šåˆ©ç”¨ç‰©ç†æ–¹ç¨‹ç”Ÿæˆ"é«˜æ¸©ç¼ºæ°§"è‡´æ­»åœºæ™¯çš„åˆæˆæ­£æ ·æœ¬ï¼Œ
     æ‰©å……çº¢å…‰è®­ç»ƒé›†ä¸­ä¸å–€å·¦æ­»äº¡æ¨¡å¼ç›¸ä¼¼çš„æç«¯æ ·æœ¬ã€‚
  B. ä¸¤é˜¶æ®µè¿ç§»å­¦ä¹ ï¼š
     é˜¶æ®µ1 â€” åœ¨çº¢å…‰(+åˆæˆ)æ•°æ®ä¸Šé¢„è®­ç»ƒ XGBoostï¼›
     é˜¶æ®µ2 â€” å–å–€å·¦ 10%ï¼ˆåˆ†å±‚æŠ½æ ·ï¼‰æ•°æ®å¾®è°ƒï¼Œå‰©ä½™ 90% ä½œä¸ºæµ‹è¯•ã€‚

è¾“å‡ºç›®å½•ï¼šoutput_v1.2/
"""
import sys
import warnings
import json
from datetime import datetime
from pathlib import Path
from copy import deepcopy

import numpy as np
import pandas as pd
import xgboost as xgb
import shap
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix,
    roc_curve, precision_recall_curve, average_precision_score,
)
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

from config import (
    FEATURED_HONGGUANG, FEATURED_KAZUO,
    META_COLS, LABEL_COLS, CUMULATIVE_LABEL_COLS,
    CV_FOLDS, SHAP_TOP_N, RANDOM_SEED,
)

warnings.filterwarnings("ignore", category=UserWarning)

# ============================================================
# è·¯å¾„ä¸å¸¸é‡
# ============================================================
SCRIPT_DIR = Path(__file__).resolve().parent
OUTPUT_DIR = SCRIPT_DIR / "output_v1.2"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
V1_OUTPUT = SCRIPT_DIR / "output"
V11_OUTPUT = SCRIPT_DIR / "output_v1.1"

# PINN ç‰©ç†å‚æ•°
PINN_PARAMS = {
    "K_La": 1.35815, "R_fish_base": 1.50715, "alpha_T": 0.0338,
    "R_bio": 0.751, "P_photo_rate": 0.06935, "T_ref": 25.0,
}

TASKS = {
    "è”¬èœç—…å®³": {"label_col": "è”¬èœ_ç—…å®³æ¬¡æ•°", "description": "è”¬èœæ˜¯å¦å‘ç”Ÿç—…å®³ï¼ˆäºŒåˆ†ç±»ï¼‰"},
    "é±¼ç±»æ­»äº¡": {"label_col": "é±¼_æ­»äº¡æ•°é‡", "description": "é±¼ç±»æ˜¯å¦å‘ç”Ÿæ­»äº¡ï¼ˆäºŒåˆ†ç±»ï¼‰"},
}

# åŸŸåç§»ç‰¹å¾
DOMAIN_SHIFT_FEATURES = [
    "èƒ½è€—km/h", "ç§æ¤åºŠ1æ¶²ä½ä¸Šé™è·ç§æ¤åºŠè¡¨é¢è·ç¦»cm",
    "ç§æ¤åºŠ2æ¶²ä½ä¸Šé™è·ç§æ¤åºŠè¡¨é¢è·ç¦»cm",
]

# é˜¶æ®µ1ï¼ˆé¢„è®­ç»ƒï¼‰ï¼šè¾ƒå¼±æ­£åˆ™åŒ–ï¼Œå¤šå­¦çº¢å…‰çŸ¥è¯†
XGBOOST_PRETRAIN = {
    "n_estimators": 300, "max_depth": 5, "learning_rate": 0.05,
    "subsample": 0.8, "colsample_bytree": 0.8, "min_child_weight": 5,
    "gamma": 0.1, "reg_alpha": 0.1, "reg_lambda": 1.0,
    "random_state": RANDOM_SEED, "n_jobs": -1, "eval_metric": "logloss",
    "early_stopping_rounds": 30,
}

# é˜¶æ®µ2ï¼ˆå¾®è°ƒï¼‰ï¼šæ›´å¼ºæ­£åˆ™åŒ–ï¼Œå­¦å–€å·¦è½»é‡å·®å¼‚
XGBOOST_FINETUNE = {
    "n_estimators": 150, "max_depth": 3, "learning_rate": 0.02,
    "subsample": 0.7, "colsample_bytree": 0.6, "min_child_weight": 15,
    "gamma": 0.5, "reg_alpha": 1.0, "reg_lambda": 5.0,
    "random_state": RANDOM_SEED, "n_jobs": -1, "eval_metric": "logloss",
    "early_stopping_rounds": 20,
}

# å–€å·¦ç”¨äºå¾®è°ƒçš„æ¯”ä¾‹
FINETUNE_RATIO = 0.10


# ============================================================
# ä¸­æ–‡å­—ä½“
# ============================================================
def setup_chinese_font():
    for fp in ["/System/Library/Fonts/STHeiti Light.ttc",
               "/System/Library/Fonts/PingFang.ttc",
               "/System/Library/Fonts/Supplemental/Songti.ttc",
               "/System/Library/Fonts/Hiragino Sans GB.ttc"]:
        try:
            fm.fontManager.addfont(fp)
            prop = fm.FontProperties(fname=fp)
            plt.rcParams["font.family"] = prop.get_name()
            plt.rcParams["axes.unicode_minus"] = False
            return
        except Exception:
            continue
    plt.rcParams["font.sans-serif"] = ["Arial Unicode MS", "SimHei", "DejaVu Sans"]
    plt.rcParams["axes.unicode_minus"] = False

setup_chinese_font()


# ============================================================
# ç‰©ç†ç‰¹å¾ï¼ˆåŒ v1.1ï¼‰
# ============================================================
def do_saturation(T):
    return 14.62 - 0.3898 * T + 0.006969 * T**2 - 5.897e-5 * T**3


def compute_physics_features(df):
    p = PINN_PARAMS
    T = df["æ°´æ¸©_æ—¥å‡"].fillna(25.0)
    DO = df["æº¶æ°§mg/L"].fillna(df["æº¶æ°§mg/L"].median())
    light = df["å…‰ç…§æ—¶é•¿h"].fillna(0.0)
    DO_sat = do_saturation(T)
    out = pd.DataFrame(index=df.index)
    out["pinn_DO_deficit"] = DO_sat - DO
    out["pinn_R_fish_T"] = p["R_fish_base"] * (1 + p["alpha_T"] * (T - p["T_ref"]))
    out["pinn_reaeration"] = p["K_La"] * (DO_sat - DO)
    out["pinn_P_photo"] = p["P_photo_rate"] * light
    out["pinn_oxygen_stress"] = (out["pinn_R_fish_T"] + p["R_bio"]
                                  - out["pinn_reaeration"] - out["pinn_P_photo"])
    out["pinn_DO_margin"] = DO - 2.0
    out["pinn_DO_sat_ratio"] = DO / DO_sat.clip(lower=1.0)
    return out


def compute_temporal_features(df):
    out = pd.DataFrame(index=df.index)
    T = df["æ°´æ¸©_æ—¥å‡"].fillna(25.0)
    T_air = df["æ°”æ¸©_æ—¥å‡"].fillna(20.0)
    DO = df["æº¶æ°§mg/L"].fillna(df["æº¶æ°§mg/L"].median())
    roll3 = df.get("æ°´æ¸©_æ—¥å‡_roll3d_mean")
    out["trend_æ°´æ¸©_3d"] = (T - roll3.fillna(T)) if roll3 is not None else 0.0
    do_ch = df.get("æº¶æ°§_å˜åŒ–ç‡")
    out["DO_volatility"] = do_ch.fillna(0).abs() if do_ch is not None else 0.0
    out["æ°´æ°”æ¸©_äº¤äº’"] = T * T_air / 100.0
    out["æ°´æ¸©_åç¦»é€‚å®œ"] = np.where(T > 28, T - 28, np.where(T < 18, 18 - T, 0))
    out["é«˜æ¸©å¤©æ•°_7d"] = (T > 28).astype(float).rolling(7, min_periods=1).sum().values
    out["ä½æ°§å¤©æ•°_7d"] = (DO < 5).astype(float).rolling(7, min_periods=1).sum().values
    roll3_std = df.get("æ°´æ¸©_æ—¥å‡_roll3d_std")
    out["æ°´æ¸©æ³¢åŠ¨_åŠ é€Ÿåº¦"] = roll3_std.fillna(0).diff().fillna(0) if roll3_std is not None else 0.0
    return out


# ============================================================
# PINN åäº‹å®åˆæˆ
# ============================================================
def generate_counterfactual_samples(df_source, label_col, n_synthetic=500):
    """
    åˆ©ç”¨ PINN ç‰©ç†æ–¹ç¨‹ç”Ÿæˆåˆæˆ"é«˜æ¸©ç¼ºæ°§è‡´æ­»"æ­£æ ·æœ¬ã€‚

    ç­–ç•¥ï¼š
      1. ä»çº¢å…‰æ•°æ®ä¸­é€‰å–å®é™…æ­»äº¡äº‹ä»¶å‘ç”Ÿæ—¶çš„æ ·æœ¬ä½œä¸ºæ¨¡æ¿
      2. å¯¹æ¨¡æ¿è¿›è¡Œç‰©ç†ä¸€è‡´çš„æ‰°åŠ¨ï¼š
         - æ°´æ¸©å‡é«˜ 1~5â„ƒï¼ˆæ¨¡æ‹Ÿæç«¯é«˜æ¸©ï¼‰
         - æº¶æ°§æŒ‰ç‰©ç†æ–¹ç¨‹ä¸‹é™ï¼ˆDO_sat é™ä½ + è€—æ°§å¢åŠ ï¼‰
         - å…‰ç…§æ—¶é•¿éšæœºç¼©çŸ­ï¼ˆæ¨¡æ‹Ÿé˜´å¤©/è®¾å¤‡æ•…éšœï¼‰
      3. æ ‡è®°ä¸ºæ­£æ ·æœ¬ï¼ˆæœ‰æ­»äº¡ï¼‰
      4. å¯¹éæ­»äº¡æ ·æœ¬ä¹Ÿæ–½åŠ ç‰©ç†æç«¯æ¡ä»¶ä½œä¸ºè¡¥å……

    Returns:
        df_synthetic: åˆæˆæ ·æœ¬ DataFrameï¼ˆä¸åŸå§‹åŒç»“æ„ï¼‰
    """
    p = PINN_PARAMS
    rng = np.random.RandomState(RANDOM_SEED)

    y = (df_source[label_col] > 0).astype(int)

    # æ¨¡æ¿1ï¼šä»å®é™…æ­»äº¡äº‹ä»¶ä¸­é‡‡æ ·
    death_mask = y == 1
    if death_mask.sum() > 0:
        templates_death = df_source[death_mask].sample(
            n=min(n_synthetic // 2, death_mask.sum()),
            replace=True, random_state=RANDOM_SEED
        ).copy()
    else:
        templates_death = pd.DataFrame()

    # æ¨¡æ¿2ï¼šä»é«˜æ°´æ¸©çš„éæ­»äº¡äº‹ä»¶ä¸­é‡‡æ ·ï¼ˆæ¨¡æ‹Ÿ"å·®ä¸€ç‚¹å°±æ­»äº†"åœºæ™¯ï¼‰
    high_temp_mask = (y == 0) & (df_source["æ°´æ¸©_æ—¥å‡"] > df_source["æ°´æ¸©_æ—¥å‡"].quantile(0.75))
    n_from_negative = n_synthetic - len(templates_death)
    if high_temp_mask.sum() > 0 and n_from_negative > 0:
        templates_neg = df_source[high_temp_mask].sample(
            n=min(n_from_negative, high_temp_mask.sum()),
            replace=True, random_state=RANDOM_SEED + 1
        ).copy()
    else:
        templates_neg = pd.DataFrame()

    templates = pd.concat([templates_death, templates_neg], ignore_index=True)
    if len(templates) == 0:
        print("  âš ï¸ æ— æ³•ç”Ÿæˆåˆæˆæ ·æœ¬ï¼ˆæ— æ¨¡æ¿ï¼‰")
        return pd.DataFrame()

    print(f"  ğŸ§ª åˆæˆæ¨¡æ¿: {len(templates_death)} æ¥è‡ªçœŸå®æ­»äº¡, "
          f"{len(templates_neg)} æ¥è‡ªé«˜æ¸©éæ­»äº¡")

    # ç‰©ç†ä¸€è‡´æ‰°åŠ¨
    delta_T = rng.uniform(1.0, 5.0, size=len(templates))    # å‡æ¸© 1~5â„ƒ
    light_factor = rng.uniform(0.3, 0.8, size=len(templates))  # å…‰ç…§ç¼©çŸ­

    syn = templates.copy()

    # æ°´æ¸©æå‡
    syn["æ°´æ¸©_æ—¥å‡"] = syn["æ°´æ¸©_æ—¥å‡"] + delta_T
    syn["æœ€é«˜æ°´æ¸©â„ƒ"] = syn["æœ€é«˜æ°´æ¸©â„ƒ"] + delta_T
    syn["æœ€ä½æ°´æ¸©â„ƒ"] = syn["æœ€ä½æ°´æ¸©â„ƒ"] + delta_T * 0.5

    # å…‰ç…§ç¼©çŸ­
    syn["å…‰ç…§æ—¶é•¿h"] = syn["å…‰ç…§æ—¶é•¿h"] * light_factor

    # æº¶æ°§æŒ‰ç‰©ç†æ–¹ç¨‹æ ¡æ­£
    T_new = syn["æ°´æ¸©_æ—¥å‡"].values
    DO_sat_new = do_saturation(T_new)
    R_fish_new = p["R_fish_base"] * (1 + p["alpha_T"] * (T_new - p["T_ref"]))
    P_photo_new = p["P_photo_rate"] * syn["å…‰ç…§æ—¶é•¿h"].values

    # æ–° DO = DO_sat - (R_fish + R_bio - P_photo) / K_La
    # è¿™æ˜¯ç¨³æ€è¿‘ä¼¼ï¼šdDO/dt â‰ˆ 0 æ—¶çš„å¹³è¡¡ç‚¹
    DO_equilibrium = DO_sat_new - (R_fish_new + p["R_bio"] - P_photo_new) / p["K_La"]
    DO_equilibrium = np.clip(DO_equilibrium, 0.5, DO_sat_new)  # ç‰©ç†çº¦æŸ

    if "æº¶æ°§mg/L" in syn.columns:
        # å®é™… DO å–å½“å‰å€¼å’Œå¹³è¡¡å€¼çš„è¾ƒä½è€…ï¼ˆæ¶åŒ–ï¼‰
        syn["æº¶æ°§mg/L"] = np.minimum(syn["æº¶æ°§mg/L"].values, DO_equilibrium)
        # ç»™ä¸€äº›éšæœºæ‰°åŠ¨
        syn["æº¶æ°§mg/L"] = syn["æº¶æ°§mg/L"] - rng.uniform(0, 1.5, size=len(syn))
        syn["æº¶æ°§mg/L"] = syn["æº¶æ°§mg/L"].clip(lower=0.5)

    # æ ‡è®°ä¸ºæ­£æ ·æœ¬
    syn[label_col] = 1

    # æ·»åŠ æ ‡è®°åˆ—
    syn["_is_synthetic"] = True

    # æ»šåŠ¨ç‰¹å¾å°±ç›´æ¥ç»§æ‰¿æ¨¡æ¿çš„ï¼ˆè¿‘ä¼¼åˆç†ï¼‰
    print(f"  âœ… ç”Ÿæˆ {len(syn)} ä¸ª PINN åäº‹å®åˆæˆæ­£æ ·æœ¬")
    print(f"     æ°´æ¸©: {syn['æ°´æ¸©_æ—¥å‡'].mean():.1f} Â± {syn['æ°´æ¸©_æ—¥å‡'].std():.1f}â„ƒ"
          f"  (åŸå§‹: {templates['æ°´æ¸©_æ—¥å‡'].mean():.1f}â„ƒ)")
    if "æº¶æ°§mg/L" in syn.columns:
        print(f"     æº¶æ°§: {syn['æº¶æ°§mg/L'].mean():.1f} Â± {syn['æº¶æ°§mg/L'].std():.1f} mg/L"
              f"  (åŸå§‹: {templates['æº¶æ°§mg/L'].mean():.1f} mg/L)")

    return syn


# ============================================================
# æ•°æ®åŠ è½½ + ç‰¹å¾å¢å¼º + åˆæˆæ•°æ®
# ============================================================
def load_all_data(task_config, use_synthetic=True, n_synthetic=500):
    """
    åŠ è½½å¹¶å¢å¼ºå…¨éƒ¨æ•°æ®ã€‚

    Returns:
        df_hg_full: çº¢å…‰å…¨é›†ï¼ˆå«åˆæˆï¼‰+ å¢å¼ºç‰¹å¾
        df_kz_full: å–€å·¦å…¨é›† + å¢å¼ºç‰¹å¾
        feature_names: ç‰¹å¾å
    """
    label_col = task_config["label_col"]

    df_hg = pd.read_csv(FEATURED_HONGGUANG, parse_dates=["æ—¥æœŸ"])
    df_kz = pd.read_csv(FEATURED_KAZUO, parse_dates=["æ—¥æœŸ"])
    print(f"[çº¢å…‰] {df_hg.shape[0]} è¡Œ | [å–€å·¦] {df_kz.shape[0]} è¡Œ")

    # ====== PINN åäº‹å®åˆæˆ ======
    if use_synthetic:
        print(f"\nğŸ§¬ PINN åäº‹å®åˆæˆæ­£æ ·æœ¬...")
        syn = generate_counterfactual_samples(df_hg, label_col, n_synthetic)
        if len(syn) > 0:
            # ç¡®ä¿åˆæˆæ•°æ®æœ‰ç›¸åŒåˆ—
            for c in df_hg.columns:
                if c not in syn.columns:
                    syn[c] = 0
            syn = syn[df_hg.columns.tolist() + ["_is_synthetic"]]
            df_hg["_is_synthetic"] = False
            df_hg = pd.concat([df_hg, syn[df_hg.columns]], ignore_index=True)
            print(f"  çº¢å…‰æ€»æ ·æœ¬: {len(df_hg)} (å« {len(syn)} åˆæˆ)")

    # ====== å¢åŠ ç‰©ç† + æ—¶åºç‰¹å¾ ======
    for name, df in [("çº¢å…‰", df_hg), ("å–€å·¦", df_kz)]:
        phys = compute_physics_features(df)
        temp = compute_temporal_features(df)
        for c in phys.columns:
            df[c] = phys[c].values
        for c in temp.columns:
            df[c] = temp[c].values

    print(f"  âœ… +{len(compute_physics_features(df_hg).columns)} ç‰©ç† "
          f"+{len(compute_temporal_features(df_hg).columns)} æ—¶åºç‰¹å¾")

    # ====== ç¡®å®šç‰¹å¾åˆ— ======
    exclude = set(META_COLS + LABEL_COLS + CUMULATIVE_LABEL_COLS
                  + DOMAIN_SHIFT_FEATURES + ["_is_synthetic", "_is_augmented", "_scenario"])
    feature_cols = [c for c in df_hg.columns if c not in exclude]

    # å¯¹é½åˆ—
    for c in feature_cols:
        if c not in df_kz.columns:
            df_kz[c] = 0

    return df_hg, df_kz, feature_cols


def prepare_Xy(df, feature_cols, label_col):
    """æå– X, y å¹¶å¤„ç†ç¼ºå¤±"""
    X = df[feature_cols].copy()
    y = (df[label_col] > 0).astype(int)
    for col in X.columns:
        if X[col].isnull().any():
            X[col] = X[col].fillna(X[col].median())
    X = X.replace([np.inf, -np.inf], np.nan)
    for col in X.columns:
        if X[col].isnull().any():
            X[col] = X[col].fillna(0)
    return X, y


# ============================================================
# åŸŸè‡ªé€‚åº”æƒé‡ï¼ˆåŒ v1.1ï¼‰
# ============================================================
def compute_domain_weights(X_src, X_tgt):
    common = [c for c in X_src.columns if c in X_tgt.columns]
    scaler = StandardScaler()
    s = scaler.fit_transform(X_src[common].fillna(0))
    t = scaler.transform(X_tgt[common].fillna(0))
    n_comp = min(10, s.shape[1])
    pca = PCA(n_components=n_comp, random_state=RANDOM_SEED)
    sp = pca.fit_transform(s)
    tp = pca.transform(t)
    centroid = tp.mean(axis=0)
    dist = np.sqrt(((sp - centroid) ** 2).sum(axis=1))
    sigma = np.median(dist) + 1e-8
    w = np.exp(-0.5 * (dist / sigma) ** 2)
    w = w / w.mean()
    w = np.clip(w, 0.1, 5.0)
    return w


# ============================================================
# é˜ˆå€¼æœç´¢
# ============================================================
def find_optimal_threshold(y_true, y_prob):
    best_f1, best_thr = 0, 0.5
    for thr in np.arange(0.05, 0.9, 0.01):
        y_pred = (y_prob >= thr).astype(int)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        if f1 > best_f1:
            best_f1 = f1
            best_thr = thr
    return best_thr, best_f1


# ============================================================
# è¯„ä¼°
# ============================================================
def evaluate(model, X, y, name, task_name, threshold=0.5):
    y_prob = model.predict_proba(X)[:, 1]
    y_pred = (y_prob >= threshold).astype(int)

    m = {
        "æ•°æ®é›†": name, "ä»»åŠ¡": task_name, "é˜ˆå€¼": threshold,
        "æ ·æœ¬æ•°": len(y), "æ­£æ ·æœ¬æ•°": int(y.sum()),
        "æ­£æ ·æœ¬æ¯”ä¾‹": f"{y.mean()*100:.1f}%",
        "Accuracy": accuracy_score(y, y_pred),
        "Precision": precision_score(y, y_pred, zero_division=0),
        "Recall": recall_score(y, y_pred, zero_division=0),
        "F1-Score": f1_score(y, y_pred, zero_division=0),
        "ROC-AUC": roc_auc_score(y, y_prob) if y.nunique() > 1 else 0,
        "AP": average_precision_score(y, y_prob) if y.nunique() > 1 else 0,
    }

    print(f"\n{'â”€'*55}")
    print(f"ğŸ“ˆ {name} ({task_name}) | thr={threshold:.2f}")
    print(f"{'â”€'*55}")
    for k, v in m.items():
        print(f"  {k:25s}: {v:.4f}" if isinstance(v, float) else f"  {k:25s}: {v}")

    cm = confusion_matrix(y, y_pred)
    print(f"\n  TN={cm[0,0]:5d}  FP={cm[0,1]:5d}")
    print(f"  FN={cm[1,0]:5d}  TP={cm[1,1]:5d}")

    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle(f"[v1.2] {task_name} â€” {name}", fontsize=14, fontweight="bold")

    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=axes[0],
                xticklabels=["æ— ", "æœ‰"], yticklabels=["æ— ", "æœ‰"])
    axes[0].set_xlabel("é¢„æµ‹"); axes[0].set_ylabel("å®é™…"); axes[0].set_title("æ··æ·†çŸ©é˜µ")

    if y.nunique() > 1:
        fpr, tpr, _ = roc_curve(y, y_prob)
        axes[1].plot(fpr, tpr, "b-", lw=2, label=f"AUC={m['ROC-AUC']:.4f}")
        axes[1].plot([0, 1], [0, 1], "k--", alpha=0.3)
        axes[1].set_xlabel("FPR"); axes[1].set_ylabel("TPR"); axes[1].set_title("ROC"); axes[1].legend()

        prec_arr, rec_arr, _ = precision_recall_curve(y, y_prob)
        axes[2].plot(rec_arr, prec_arr, "r-", lw=2, label=f"AP={m['AP']:.4f}")
        axes[2].axhline(y=y.mean(), color="gray", ls="--", alpha=0.5, label="éšæœº")
        axes[2].set_xlabel("Recall"); axes[2].set_ylabel("Precision"); axes[2].set_title("PR"); axes[2].legend()

    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / f"eval_{task_name}_{name}.png", dpi=150, bbox_inches="tight")
    plt.close()

    return m


# ============================================================
# SHAP åˆ†æ
# ============================================================
def shap_analysis(model, X, feature_names, task_name, dataset_name=""):
    explainer = shap.TreeExplainer(model)
    X_s = X.sample(min(5000, len(X)), random_state=RANDOM_SEED) if len(X) > 5000 else X
    sv = explainer.shap_values(X_s)

    fig, ax = plt.subplots(figsize=(12, 8))
    shap.summary_plot(sv, X_s, feature_names=feature_names, max_display=SHAP_TOP_N, show=False)
    plt.title(f"[v1.2] {task_name} â€” SHAP (Top {SHAP_TOP_N})", fontsize=14)
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / f"shap_summary_{task_name}_{dataset_name}.png", dpi=150, bbox_inches="tight")
    plt.close()

    fig, ax = plt.subplots(figsize=(10, 8))
    shap.summary_plot(sv, X_s, feature_names=feature_names, plot_type="bar", max_display=SHAP_TOP_N, show=False)
    plt.title(f"[v1.2] {task_name} â€” SHAP å‡å€¼", fontsize=14)
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / f"shap_bar_{task_name}_{dataset_name}.png", dpi=150, bbox_inches="tight")
    plt.close()

    mean_abs = np.abs(sv).mean(axis=0)
    imp = pd.DataFrame({"ç‰¹å¾": feature_names, "SHAPå‡å€¼": mean_abs}).sort_values("SHAPå‡å€¼", ascending=False).reset_index(drop=True)
    imp.index += 1; imp.index.name = "æ’å"
    imp.to_csv(OUTPUT_DIR / f"shap_importance_{task_name}_{dataset_name}.csv")

    print(f"\n  ğŸ“‹ Top 10:")
    print(imp.head(10).to_string())
    return imp


# ============================================================
# åŠ è½½å†å²æŠ¥å‘Š
# ============================================================
def load_prev_report(task_name, version, output_dir):
    patterns = [f"report_{task_name}.json", f"report_{task_name}_{version}.json"]
    for pat in patterns:
        p = output_dir / pat
        if p.exists():
            with open(p, "r", encoding="utf-8") as f:
                return json.load(f)
    return None


# ============================================================
# ä¸»æµç¨‹ï¼šä¸€ä¸ªä»»åŠ¡
# ============================================================
def run_task(task_name, task_config, df_hg, df_kz, feature_cols):
    label_col = task_config["label_col"]

    print(f"\n{'#'*60}")
    print(f"## [v1.2] {task_name} â€” {task_config['description']}")
    print(f"{'#'*60}")

    # ====== å‡†å¤‡æ•°æ® ======
    X_hg, y_hg = prepare_Xy(df_hg, feature_cols, label_col)
    X_kz, y_kz = prepare_Xy(df_kz, feature_cols, label_col)

    print(f"\n  çº¢å…‰: {len(X_hg)} æ ·æœ¬, æ­£={y_hg.sum()} ({y_hg.mean()*100:.1f}%)")
    print(f"  å–€å·¦: {len(X_kz)} æ ·æœ¬, æ­£={y_kz.sum()} ({y_kz.mean()*100:.1f}%)")

    # ====== å–€å·¦åˆ†å±‚æ‹†åˆ†ï¼š10% å¾®è°ƒ + 90% æµ‹è¯• ======
    if y_kz.sum() >= 5:
        X_kz_ft, X_kz_test, y_kz_ft, y_kz_test = train_test_split(
            X_kz, y_kz, test_size=1.0 - FINETUNE_RATIO,
            random_state=RANDOM_SEED, stratify=y_kz
        )
    else:
        # æ­£æ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•åˆ†å±‚æŠ½æ ·
        X_kz_ft = X_kz.sample(frac=FINETUNE_RATIO, random_state=RANDOM_SEED)
        y_kz_ft = y_kz.loc[X_kz_ft.index]
        X_kz_test = X_kz.drop(X_kz_ft.index)
        y_kz_test = y_kz.drop(y_kz_ft.index)

    print(f"\n  ğŸ“Œ è¿ç§»å­¦ä¹ æ‹†åˆ†:")
    print(f"    å–€å·¦å¾®è°ƒé›†: {len(X_kz_ft)} æ ·æœ¬, æ­£={y_kz_ft.sum()}")
    print(f"    å–€å·¦æµ‹è¯•é›†: {len(X_kz_test)} æ ·æœ¬, æ­£={y_kz_test.sum()}")

    # ====== çº¢å…‰æ‹†åˆ†è®­ç»ƒ/éªŒè¯ ======
    X_hg_train, X_hg_val, y_hg_train, y_hg_val = train_test_split(
        X_hg, y_hg, test_size=0.2, random_state=RANDOM_SEED, stratify=y_hg
    )

    # åŸŸè‡ªé€‚åº”æƒé‡
    print("\nğŸ¯ åŸŸè‡ªé€‚åº”æƒé‡...")
    domain_w = compute_domain_weights(X_hg_train, X_kz)
    print(f"    mean={domain_w.mean():.3f}, std={domain_w.std():.3f}")

    # ====== é˜¶æ®µ1ï¼šçº¢å…‰é¢„è®­ç»ƒ ======
    print(f"\n{'='*50}")
    print(f"ğŸš€ é˜¶æ®µ1: çº¢å…‰(+åˆæˆ)é¢„è®­ç»ƒ")
    print(f"{'='*50}")

    n_pos = y_hg_train.sum(); n_neg = len(y_hg_train) - n_pos
    spw = n_neg / max(n_pos, 1)
    print(f"  scale_pos_weight = {spw:.2f}")

    params1 = XGBOOST_PRETRAIN.copy()
    es1 = params1.pop("early_stopping_rounds", 30)

    model_pretrain = xgb.XGBClassifier(
        scale_pos_weight=spw, use_label_encoder=False, **params1,
    )
    model_pretrain.fit(
        X_hg_train, y_hg_train, sample_weight=domain_w,
        eval_set=[(X_hg_val, y_hg_val)], verbose=50,
    )

    # é¢„è®­ç»ƒ éªŒè¯é›†è¯„ä¼°
    pre_val = evaluate(model_pretrain, X_hg_val, y_hg_val, "é¢„è®­ç»ƒ_éªŒè¯é›†_çº¢å…‰", task_name)
    # é¢„è®­ç»ƒ å–€å·¦æµ‹è¯•é›†è¯„ä¼°ï¼ˆä½œä¸º baselineï¼‰
    pre_kz = evaluate(model_pretrain, X_kz_test, y_kz_test, "é¢„è®­ç»ƒ_å–€å·¦æµ‹è¯•", task_name)

    # ====== é˜¶æ®µ2ï¼šå–€å·¦å¾®è°ƒ ======
    print(f"\n{'='*50}")
    print(f"ğŸ”§ é˜¶æ®µ2: å–€å·¦ {FINETUNE_RATIO*100:.0f}% å¾®è°ƒ")
    print(f"{'='*50}")

    # åˆå¹¶ï¼šçº¢å…‰éªŒè¯é›† + å–€å·¦å¾®è°ƒé›†ï¼ˆå°‘é‡ï¼‰ä½œä¸ºå¾®è°ƒè®­ç»ƒé›†
    # ç­–ç•¥ï¼šä¸»è¦ç”¨å–€å·¦å¾®è°ƒæ•°æ®ï¼Œè¾…ä»¥éƒ¨åˆ†çº¢å…‰æ•°æ®é˜²æ­¢ç¾éš¾æ€§é—å¿˜
    X_ft = pd.concat([X_hg_val, X_kz_ft], axis=0)
    y_ft = pd.concat([y_hg_val, y_kz_ft], axis=0)

    # ç»™å–€å·¦å¾®è°ƒæ•°æ®æ›´é«˜æƒé‡ï¼ˆ5xï¼‰
    w_ft = np.ones(len(X_ft))
    w_ft[len(X_hg_val):] = 5.0  # å–€å·¦æ ·æœ¬ 5x æƒé‡
    print(f"  å¾®è°ƒè®­ç»ƒ: {len(X_ft)} æ ·æœ¬ (çº¢å…‰éªŒè¯={len(X_hg_val)}, å–€å·¦={len(X_kz_ft)})")
    print(f"  å–€å·¦æ ·æœ¬æƒé‡: 5.0x")

    n_pos_ft = y_ft.sum(); n_neg_ft = len(y_ft) - n_pos_ft
    spw_ft = n_neg_ft / max(n_pos_ft, 1)

    params2 = XGBOOST_FINETUNE.copy()
    es2 = params2.pop("early_stopping_rounds", 20)

    # ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ booster åˆå§‹åŒ–å¾®è°ƒæ¨¡å‹
    model_finetune = xgb.XGBClassifier(
        scale_pos_weight=spw_ft, use_label_encoder=False, **params2,
    )

    # å¾®è°ƒï¼šä½¿ç”¨ xgb_model å‚æ•°è¿›è¡Œå¢é‡è®­ç»ƒï¼ˆè¿ç§»å­¦ä¹ æ ¸å¿ƒï¼‰
    model_finetune.fit(
        X_ft, y_ft, sample_weight=w_ft,
        eval_set=[(X_kz_test.head(500), y_kz_test.head(500))],  # å°é‡éªŒè¯
        xgb_model=model_pretrain.get_booster(),  # ç»§æ‰¿é¢„è®­ç»ƒæƒé‡
        verbose=50,
    )

    # ä¿å­˜æ¨¡å‹
    model_finetune.save_model(str(OUTPUT_DIR / f"xgb_{task_name}_v1.2.json"))

    # ====== é˜ˆå€¼ä¼˜åŒ–ï¼ˆåœ¨å–€å·¦å¾®è°ƒé›†ä¸Šæœç´¢ï¼‰ ======
    y_ft_prob = model_finetune.predict_proba(X_kz_ft)[:, 1]
    opt_thr, opt_f1 = find_optimal_threshold(y_kz_ft, y_ft_prob)
    print(f"\nğŸ¯ æœ€ä¼˜é˜ˆå€¼: {opt_thr:.2f} (å¾®è°ƒé›† F1={opt_f1:.4f})")

    # ====== æœ€ç»ˆè¯„ä¼° ======
    val_m = evaluate(model_finetune, X_hg_val, y_hg_val, "éªŒè¯é›†_çº¢å…‰", task_name, opt_thr)
    test_m = evaluate(model_finetune, X_kz_test, y_kz_test, "æµ‹è¯•é›†_å–€å·¦_90%", task_name, opt_thr)

    # SHAP
    imp = shap_analysis(model_finetune, X_hg_val, list(feature_cols), task_name, "éªŒè¯é›†")

    # ====== ä¸‰ç‰ˆå¯¹æ¯” ======
    v10 = load_prev_report(task_name, "v1.0", V1_OUTPUT)
    v11 = load_prev_report(task_name, "v1.1", V11_OUTPUT)

    comparison = {"ä»»åŠ¡": task_name}

    versions = []
    if v10:
        versions.append(("v1.0", v10.get("ç‹¬ç«‹æµ‹è¯•é›†_å–€å·¦", {})))
    if v11:
        versions.append(("v1.1", v11.get("ç‹¬ç«‹æµ‹è¯•é›†_å–€å·¦", {})))
    versions.append(("v1.2", test_m))

    print(f"\n{'='*60}")
    print(f"ğŸ“Š ç‰ˆæœ¬å¯¹æ¯” â€” {task_name} (å–€å·¦æµ‹è¯•é›†)")
    print(f"{'='*60}")
    for ver, data in versions:
        auc = data.get("ROC-AUC", "N/A")
        f1v = data.get("F1-Score", "N/A")
        rec = data.get("Recall", "N/A")
        auc_s = f"{auc:.4f}" if isinstance(auc, float) else str(auc)
        f1_s = f"{f1v:.4f}" if isinstance(f1v, float) else str(f1v)
        rec_s = f"{rec:.4f}" if isinstance(rec, float) else str(rec)
        print(f"  {ver}: AUC={auc_s}, F1={f1_s}, Recall={rec_s}")
        comparison[ver] = {"AUC": auc, "F1": f1v, "Recall": rec}

    # ====== æŠ¥å‘Š ======
    report = {
        "ç‰ˆæœ¬": "v1.2",
        "ä»»åŠ¡": task_name,
        "æè¿°": task_config["description"],
        "æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "æ”¹è¿›ç‚¹": [
            f"PINN åäº‹å®åˆæˆæ­£æ ·æœ¬",
            f"ä¸¤é˜¶æ®µè¿ç§»å­¦ä¹ : çº¢å…‰é¢„è®­ç»ƒ â†’ å–€å·¦ {FINETUNE_RATIO*100:.0f}% å¾®è°ƒ",
            "åŸŸè‡ªé€‚åº”å®ä¾‹åŠ æƒ",
            "PINN ç‰©ç†ç‰¹å¾ + æ—¶åºè¶‹åŠ¿ç‰¹å¾",
            f"F1-æœ€ä¼˜é˜ˆå€¼: {opt_thr:.2f}",
        ],
        "è¿ç§»å­¦ä¹ ": {
            "å–€å·¦å¾®è°ƒæ¯”ä¾‹": FINETUNE_RATIO,
            "å–€å·¦å¾®è°ƒæ ·æœ¬æ•°": int(len(X_kz_ft)),
            "å–€å·¦æµ‹è¯•æ ·æœ¬æ•°": int(len(X_kz_test)),
            "é¢„è®­ç»ƒ_å–€å·¦AUC": pre_kz.get("ROC-AUC"),
            "å¾®è°ƒå_å–€å·¦AUC": test_m.get("ROC-AUC"),
        },
        "éªŒè¯é›†": {k: v for k, v in val_m.items() if isinstance(v, (int, float, str))},
        "æµ‹è¯•é›†_å–€å·¦": {k: v for k, v in test_m.items() if isinstance(v, (int, float, str))},
        "Top10ç‰¹å¾": imp.head(10)["ç‰¹å¾"].tolist(),
        "ç‰ˆæœ¬å¯¹æ¯”": comparison,
    }

    with open(OUTPUT_DIR / f"report_{task_name}_v1.2.json", "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)

    return model_finetune, report


# ============================================================
# å…¥å£
# ============================================================
def main():
    print("=" * 70)
    print("  ğŸŸğŸ¥¬ ç—…å®³é¢„æµ‹ v1.2 â€” PINN åäº‹å®åˆæˆ + è¿ç§»å­¦ä¹ ")
    print("=" * 70)
    print(f"  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  è¾“å‡º: {OUTPUT_DIR}")
    print()

    # åŠ è½½æ•°æ®ï¼ˆç»Ÿä¸€å¤„ç†ï¼‰
    all_reports = {}

    for task_name, task_config in TASKS.items():
        # é±¼ç±»æ­»äº¡æ‰éœ€è¦åˆæˆæ•°æ®ï¼Œè”¬èœç—…å®³å·²ç»å¤Ÿå¥½äº†
        use_syn = (task_name == "é±¼ç±»æ­»äº¡")
        n_syn = 500 if use_syn else 0

        df_hg, df_kz, feature_cols = load_all_data(
            task_config, use_synthetic=use_syn, n_synthetic=n_syn
        )
        model, report = run_task(task_name, task_config, df_hg, df_kz, feature_cols)
        all_reports[task_name] = report

    # æ±‡æ€»
    with open(OUTPUT_DIR / "summary_v1.2.json", "w", encoding="utf-8") as f:
        json.dump(all_reports, f, ensure_ascii=False, indent=2, default=str)

    print(f"\n\n{'='*70}")
    print(f"  âœ… v1.2 å®Œæˆ!")
    print(f"{'='*70}")
    for p in sorted(OUTPUT_DIR.iterdir()):
        print(f"    {p.name:55s} ({p.stat().st_size/1024:.1f} KB)")

    print(f"\n{'='*70}")
    print(f"  ğŸ“Š ç‰ˆæœ¬æ¼”è¿›æ€»ç»“ (å–€å·¦æµ‹è¯•é›†)")
    print(f"{'='*70}")
    for tn, rep in all_reports.items():
        vc = rep.get("ç‰ˆæœ¬å¯¹æ¯”", {})
        print(f"\n  {tn}:")
        for ver in ["v1.0", "v1.1", "v1.2"]:
            d = vc.get(ver, {})
            if d:
                print(f"    {ver}: AUC={d.get('AUC','?')}, F1={d.get('F1','?')}")


if __name__ == "__main__":
    main()

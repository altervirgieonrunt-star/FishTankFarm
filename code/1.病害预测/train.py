"""
XGBoost è®­ç»ƒä¸è¯„ä¼°æ¨¡å—
- è®­ç»ƒ XGBoost äºŒåˆ†ç±»æ¨¡å‹
- 5æŠ˜äº¤å‰éªŒè¯
- ç‹¬ç«‹æµ‹è¯•é›†è¯„ä¼°ï¼ˆçº¢å…‰è®­ç»ƒ â†’ å–€å·¦æµ‹è¯•ï¼‰
- SHAP å¯è§£é‡Šæ€§åˆ†æ
- è¾“å‡ºæŠ¥å‘Šä¸å¯è§†åŒ–
"""
import sys
import warnings
import json
from datetime import datetime

import numpy as np
import pandas as pd
import xgboost as xgb
import shap
import joblib
import matplotlib
matplotlib.use("Agg")  # æ— å¤´æ¨¡å¼
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, precision_recall_curve, average_precision_score,
)
from sklearn.model_selection import StratifiedKFold

from config import XGBOOST_PARAMS, CV_FOLDS, SHAP_TOP_N, RANDOM_SEED, OUTPUT_DIR, TASKS
from data_loader import load_train_test

warnings.filterwarnings("ignore", category=UserWarning)

# ============================================================
# ä¸­æ–‡å­—ä½“è®¾ç½®
# ============================================================
def setup_chinese_font():
    """é…ç½® matplotlib ä¸­æ–‡æ˜¾ç¤º"""
    font_candidates = [
        "/System/Library/Fonts/STHeiti Light.ttc",
        "/System/Library/Fonts/PingFang.ttc",
        "/System/Library/Fonts/Supplemental/Songti.ttc",
        "/System/Library/Fonts/Hiragino Sans GB.ttc",
    ]
    for fp in font_candidates:
        try:
            fm.fontManager.addfont(fp)
            prop = fm.FontProperties(fname=fp)
            plt.rcParams["font.family"] = prop.get_name()
            plt.rcParams["axes.unicode_minus"] = False
            print(f"âœ… ä½¿ç”¨ä¸­æ–‡å­—ä½“: {prop.get_name()}")
            return
        except Exception:
            continue
    # fallback: å°è¯•ç³»ç»Ÿé»˜è®¤
    plt.rcParams["font.sans-serif"] = ["Arial Unicode MS", "SimHei", "DejaVu Sans"]
    plt.rcParams["axes.unicode_minus"] = False
    print("âš ï¸ æœªæ‰¾åˆ°ç†æƒ³ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨ fallback")

setup_chinese_font()


# ============================================================
# è®­ç»ƒ
# ============================================================
def train_xgboost(X_train, y_train, X_val, y_val, task_name: str):
    """è®­ç»ƒ XGBoost æ¨¡å‹"""
    # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆå¤„ç†ä¸å¹³è¡¡ï¼‰
    n_pos = y_train.sum()
    n_neg = len(y_train) - n_pos
    scale_pos_weight = n_neg / max(n_pos, 1)
    print(f"\nğŸ”§ class imbalance â€” scale_pos_weight = {scale_pos_weight:.2f}")

    params = XGBOOST_PARAMS.copy()
    early_stop = params.pop("early_stopping_rounds", 30)

    model = xgb.XGBClassifier(
        scale_pos_weight=scale_pos_weight,
        use_label_encoder=False,
        **params,
    )

    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=50,
    )

    # ä¿å­˜æ¨¡å‹
    model_path = OUTPUT_DIR / f"xgb_{task_name}.json"
    model.save_model(str(model_path))
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    return model


# ============================================================
# è¯„ä¼°
# ============================================================
def evaluate(model, X, y, dataset_name: str, task_name: str):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶ç”Ÿæˆå¯è§†åŒ–"""
    y_pred = model.predict(X)
    y_prob = model.predict_proba(X)[:, 1]

    # åŸºç¡€æŒ‡æ ‡
    metrics = {
        "æ•°æ®é›†": dataset_name,
        "ä»»åŠ¡": task_name,
        "æ ·æœ¬æ•°": len(y),
        "æ­£æ ·æœ¬æ•°": int(y.sum()),
        "æ­£æ ·æœ¬æ¯”ä¾‹": f"{y.mean()*100:.1f}%",
        "Accuracy": accuracy_score(y, y_pred),
        "Precision": precision_score(y, y_pred, zero_division=0),
        "Recall": recall_score(y, y_pred, zero_division=0),
        "F1-Score": f1_score(y, y_pred, zero_division=0),
        "ROC-AUC": roc_auc_score(y, y_prob) if y.nunique() > 1 else 0,
        "AP (Average Precision)": average_precision_score(y, y_prob) if y.nunique() > 1 else 0,
    }

    print(f"\n{'â”€'*50}")
    print(f"ğŸ“ˆ è¯„ä¼°ç»“æœ â€” {dataset_name} ({task_name})")
    print(f"{'â”€'*50}")
    for k, v in metrics.items():
        if isinstance(v, float):
            print(f"  {k:25s}: {v:.4f}")
        else:
            print(f"  {k:25s}: {v}")

    print(f"\næ··æ·†çŸ©é˜µ:")
    cm = confusion_matrix(y, y_pred)
    print(f"  TN={cm[0,0]:6d}  FP={cm[0,1]:6d}")
    print(f"  FN={cm[1,0]:6d}  TP={cm[1,1]:6d}")

    # === å¯è§†åŒ– ===

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle(f"{task_name} â€” {dataset_name}", fontsize=14, fontweight="bold")

    # 1) æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
    ax = axes[0]
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax,
                xticklabels=["æ— ", "æœ‰"], yticklabels=["æ— ", "æœ‰"])
    ax.set_xlabel("é¢„æµ‹")
    ax.set_ylabel("å®é™…")
    ax.set_title("æ··æ·†çŸ©é˜µ")

    # 2) ROC æ›²çº¿
    ax = axes[1]
    if y.nunique() > 1:
        fpr, tpr, _ = roc_curve(y, y_prob)
        ax.plot(fpr, tpr, "b-", lw=2, label=f"AUC = {metrics['ROC-AUC']:.4f}")
        ax.plot([0, 1], [0, 1], "k--", alpha=0.3)
        ax.set_xlabel("False Positive Rate")
        ax.set_ylabel("True Positive Rate")
        ax.set_title("ROC æ›²çº¿")
        ax.legend()
    else:
        ax.text(0.5, 0.5, "æ ‡ç­¾æ— å˜åŒ–\næ— æ³•ç»˜åˆ¶", ha="center", va="center")

    # 3) Precision-Recall æ›²çº¿
    ax = axes[2]
    if y.nunique() > 1:
        prec, rec, _ = precision_recall_curve(y, y_prob)
        ax.plot(rec, prec, "r-", lw=2, label=f"AP = {metrics['AP (Average Precision)']:.4f}")
        ax.set_xlabel("Recall")
        ax.set_ylabel("Precision")
        ax.set_title("Precision-Recall æ›²çº¿")
        ax.legend()
    else:
        ax.text(0.5, 0.5, "æ ‡ç­¾æ— å˜åŒ–\næ— æ³•ç»˜åˆ¶", ha="center", va="center")

    plt.tight_layout()
    fig_path = OUTPUT_DIR / f"eval_{task_name}_{dataset_name}.png"
    plt.savefig(fig_path, dpi=150, bbox_inches="tight")
    plt.close()
    print(f"ğŸ“Š è¯„ä¼°å›¾è¡¨å·²ä¿å­˜: {fig_path}")

    return metrics


# ============================================================
# äº¤å‰éªŒè¯
# ============================================================
def cross_validate(X, y, task_name: str):
    """5æŠ˜äº¤å‰éªŒè¯"""
    print(f"\n{'='*50}")
    print(f"ğŸ”„ {CV_FOLDS}æŠ˜äº¤å‰éªŒè¯ â€” {task_name}")
    print(f"{'='*50}")

    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_SEED)
    fold_metrics = []

    n_pos = y.sum()
    n_neg = len(y) - n_pos
    scale_pos_weight = n_neg / max(n_pos, 1)

    params = XGBOOST_PARAMS.copy()
    params.pop("early_stopping_rounds", None)

    for fold_i, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]
        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]

        model = xgb.XGBClassifier(
            scale_pos_weight=scale_pos_weight,
            use_label_encoder=False,
            **params,
        )
        model.fit(X_tr, y_tr, verbose=0)

        y_pred = model.predict(X_vl)
        y_prob = model.predict_proba(X_vl)[:, 1]

        fold_metrics.append({
            "Fold": fold_i,
            "F1": f1_score(y_vl, y_pred, zero_division=0),
            "Precision": precision_score(y_vl, y_pred, zero_division=0),
            "Recall": recall_score(y_vl, y_pred, zero_division=0),
            "AUC": roc_auc_score(y_vl, y_prob) if y_vl.nunique() > 1 else 0,
        })
        print(f"  Fold {fold_i}: F1={fold_metrics[-1]['F1']:.4f}, "
              f"AUC={fold_metrics[-1]['AUC']:.4f}")

    df_cv = pd.DataFrame(fold_metrics)
    print(f"\n  å¹³å‡ F1:  {df_cv['F1'].mean():.4f} Â± {df_cv['F1'].std():.4f}")
    print(f"  å¹³å‡ AUC: {df_cv['AUC'].mean():.4f} Â± {df_cv['AUC'].std():.4f}")

    return df_cv


# ============================================================
# SHAP åˆ†æ
# ============================================================
def shap_analysis(model, X, feature_names, task_name: str, dataset_name: str = ""):
    """SHAP å¯è§£é‡Šæ€§åˆ†æ"""
    print(f"\n{'='*50}")
    print(f"ğŸ” SHAP åˆ†æ â€” {task_name} ({dataset_name})")
    print(f"{'='*50}")

    # ä½¿ç”¨ TreeExplainerï¼ˆXGBoost ä¸“ç”¨ï¼Œé€Ÿåº¦å¿«ï¼‰
    explainer = shap.TreeExplainer(model)

    # å¦‚æœæ•°æ®é‡å¤ªå¤§ï¼ŒæŠ½æ ·
    if len(X) > 5000:
        X_sample = X.sample(5000, random_state=RANDOM_SEED)
        print(f"  é‡‡æ · 5000 æ¡è¿›è¡Œ SHAP åˆ†æï¼ˆåŸå§‹ {len(X)} æ¡ï¼‰")
    else:
        X_sample = X

    shap_values = explainer.shap_values(X_sample)

    # === 1. SHAP Summary Plotï¼ˆèœ‚ç¾¤å›¾ï¼‰===
    fig, ax = plt.subplots(figsize=(12, 8))
    shap.summary_plot(
        shap_values, X_sample,
        feature_names=feature_names,
        max_display=SHAP_TOP_N,
        show=False,
    )
    plt.title(f"{task_name} â€” SHAP ç‰¹å¾é‡è¦æ€§ï¼ˆTop {SHAP_TOP_N}ï¼‰", fontsize=14)
    plt.tight_layout()
    fig_path = OUTPUT_DIR / f"shap_summary_{task_name}_{dataset_name}.png"
    plt.savefig(fig_path, dpi=150, bbox_inches="tight")
    plt.close()
    print(f"  ğŸ“Š SHAP Summary Plot å·²ä¿å­˜: {fig_path}")

    # === 2. SHAP Bar Plotï¼ˆæŸ±çŠ¶å›¾ï¼‰===
    fig, ax = plt.subplots(figsize=(10, 8))
    shap.summary_plot(
        shap_values, X_sample,
        feature_names=feature_names,
        plot_type="bar",
        max_display=SHAP_TOP_N,
        show=False,
    )
    plt.title(f"{task_name} â€” SHAP å¹³å‡ç»å¯¹å€¼ï¼ˆTop {SHAP_TOP_N}ï¼‰", fontsize=14)
    plt.tight_layout()
    fig_path = OUTPUT_DIR / f"shap_bar_{task_name}_{dataset_name}.png"
    plt.savefig(fig_path, dpi=150, bbox_inches="tight")
    plt.close()
    print(f"  ğŸ“Š SHAP Bar Plot å·²ä¿å­˜: {fig_path}")

    # === 3. Top-3 ç‰¹å¾çš„ Dependence Plot ===
    mean_abs_shap = np.abs(shap_values).mean(axis=0)
    top3_idx = np.argsort(mean_abs_shap)[-3:][::-1]

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    for i, idx in enumerate(top3_idx):
        shap.dependence_plot(
            idx, shap_values, X_sample,
            feature_names=feature_names,
            ax=axes[i],
            show=False,
        )
    fig.suptitle(f"{task_name} â€” Top 3 ç‰¹å¾ä¾èµ–å›¾", fontsize=14)
    plt.tight_layout()
    fig_path = OUTPUT_DIR / f"shap_dependence_{task_name}_{dataset_name}.png"
    plt.savefig(fig_path, dpi=150, bbox_inches="tight")
    plt.close()
    print(f"  ğŸ“Š SHAP Dependence Plot å·²ä¿å­˜: {fig_path}")

    # === 4. å•æ ·æœ¬ Waterfall Plot ===
    # æŒ‘ä¸€ä¸ªæ­£æ ·æœ¬å±•ç¤ºå½’å› 
    explanation = shap.Explanation(
        values=shap_values,
        base_values=explainer.expected_value,
        data=X_sample.values,
        feature_names=feature_names,
    )
    # æ‰¾ä¸€ä¸ªæ­£é¢„æµ‹æ¦‚ç‡è¾ƒé«˜çš„æ ·æœ¬
    probs = model.predict_proba(X_sample)[:, 1]
    high_risk_idx = np.argmax(probs)

    fig, ax = plt.subplots(figsize=(12, 8))
    shap.plots.waterfall(explanation[high_risk_idx], max_display=15, show=False)
    plt.title(f"{task_name} â€” é«˜é£é™©æ ·æœ¬å½’å› åˆ†æ", fontsize=14)
    plt.tight_layout()
    fig_path = OUTPUT_DIR / f"shap_waterfall_{task_name}_{dataset_name}.png"
    plt.savefig(fig_path, dpi=150, bbox_inches="tight")
    plt.close()
    print(f"  ğŸ“Š SHAP Waterfall Plot å·²ä¿å­˜: {fig_path}")

    # === 5. è¾“å‡ºç‰¹å¾é‡è¦æ€§æ’å ===
    importance_df = pd.DataFrame({
        "ç‰¹å¾": feature_names,
        "SHAPå‡å€¼": mean_abs_shap,
    }).sort_values("SHAPå‡å€¼", ascending=False).reset_index(drop=True)
    importance_df.index += 1
    importance_df.index.name = "æ’å"

    csv_path = OUTPUT_DIR / f"shap_importance_{task_name}_{dataset_name}.csv"
    importance_df.to_csv(csv_path)
    print(f"  ğŸ“„ SHAP ç‰¹å¾æ’åå·²ä¿å­˜: {csv_path}")

    print(f"\n  ğŸ“‹ Top {SHAP_TOP_N} ç‰¹å¾:")
    print(importance_df.head(SHAP_TOP_N).to_string())

    return importance_df


# ============================================================
# ä¸»æµç¨‹
# ============================================================
def run_task(task_name: str, task_config: dict, use_augmented: bool = False):
    """è¿è¡Œä¸€ä¸ªå®Œæ•´çš„é¢„æµ‹ä»»åŠ¡"""
    print(f"\n{'#'*60}")
    print(f"## ä»»åŠ¡: {task_name} â€” {task_config['description']}")
    print(f"{'#'*60}")

    # 1. åŠ è½½æ•°æ®
    X_train, X_val, y_train, y_val, X_test, y_test, feature_names = \
        load_train_test(task_config, use_augmented=use_augmented)

    # 2. äº¤å‰éªŒè¯ï¼ˆçº¢å…‰æ•°æ®å†…éƒ¨ï¼‰
    X_all_hg = pd.concat([X_train, X_val], axis=0)
    y_all_hg = pd.concat([y_train, y_val], axis=0)
    cv_results = cross_validate(X_all_hg, y_all_hg, task_name)

    # 3. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print(f"\nğŸš€ è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    model = train_xgboost(X_train, y_train, X_val, y_val, task_name)

    # 4. è¯„ä¼°
    val_metrics = evaluate(model, X_val, y_val, "éªŒè¯é›†_çº¢å…‰", task_name)
    test_metrics = evaluate(model, X_test, y_test, "æµ‹è¯•é›†_å–€å·¦", task_name)

    # 5. SHAP åˆ†æï¼ˆåœ¨éªŒè¯é›†ä¸Šï¼‰
    importance = shap_analysis(model, X_val, feature_names, task_name, "éªŒè¯é›†")

    # 6. æ±‡æ€»æŠ¥å‘Š
    report = {
        "ä»»åŠ¡": task_name,
        "æè¿°": task_config["description"],
        "æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "æ•°æ®å¢å¼º": use_augmented,
        "äº¤å‰éªŒè¯": {
            "F1_mean": float(cv_results["F1"].mean()),
            "F1_std": float(cv_results["F1"].std()),
            "AUC_mean": float(cv_results["AUC"].mean()),
            "AUC_std": float(cv_results["AUC"].std()),
        },
        "éªŒè¯é›†": {k: v for k, v in val_metrics.items()
                   if isinstance(v, (int, float, str))},
        "ç‹¬ç«‹æµ‹è¯•é›†_å–€å·¦": {k: v for k, v in test_metrics.items()
                          if isinstance(v, (int, float, str))},
        "Top10ç‰¹å¾": importance.head(10)["ç‰¹å¾"].tolist(),
    }

    report_path = OUTPUT_DIR / f"report_{task_name}.json"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    return model, report


# ============================================================
# å…¥å£
# ============================================================
def main():
    print("=" * 70)
    print("  ğŸŸğŸ¥¬ é±¼èœå…±ç”Ÿç—…å®³é¢„æµ‹ç³»ç»Ÿ â€” XGBoost + SHAP")
    print("=" * 70)
    print(f"  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print()

    all_reports = {}
    for task_name, task_config in TASKS.items():
        model, report = run_task(task_name, task_config, use_augmented=False)
        all_reports[task_name] = report

    # ä¿å­˜æ±‡æ€»
    summary_path = OUTPUT_DIR / "summary.json"
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(all_reports, f, ensure_ascii=False, indent=2, default=str)

    print(f"\n\n{'='*70}")
    print(f"  âœ… å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"  åŒ…å«æ–‡ä»¶:")
    for p in sorted(OUTPUT_DIR.iterdir()):
        size_kb = p.stat().st_size / 1024
        print(f"    {p.name:50s} ({size_kb:.1f} KB)")


if __name__ == "__main__":
    main()

"""
ç—…å®³é¢„æµ‹ v1.1ï¼šèåˆ PINN ç‰©ç†ç‰¹å¾ + æ—¶åºè¶‹åŠ¿ + åŸŸè‡ªé€‚åº”
==========================================================
æ”¹è¿›ç‚¹ï¼š
  1. PINN ç‰©ç†ç‰¹å¾æ³¨å…¥ï¼ˆDO äºæŸã€æ¸©åº¦ä¿®æ­£è€—æ°§ã€æ°§æ°”å‹åŠ›æŒ‡æ•°ç­‰ï¼‰
  2. æ—¶åºè¶‹åŠ¿ç‰¹å¾ï¼ˆæ¸©åº¦/æº¶æ°§çŸ­æœŸè¶‹åŠ¿ã€ç´¯ç§¯é«˜æ¸©ä½æ°§å¤©æ•°ï¼‰
  3. åŸŸè‡ªé€‚åº”å®ä¾‹åŠ æƒï¼ˆåŸºäºç‰¹å¾åˆ†å¸ƒç›¸ä¼¼åº¦ï¼‰
  4. æ›´å¼ºæ­£åˆ™åŒ–ï¼ˆé™ä½è·¨åŸºåœ°è¿‡æ‹Ÿåˆï¼‰
  5. é˜ˆå€¼ä¼˜åŒ–ï¼ˆF1-optimal thresholdï¼‰

è¾“å‡ºç›®å½•ï¼šoutput_v1.1/
"""
import sys
import warnings
import json
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import xgboost as xgb
import shap
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, precision_recall_curve, average_precision_score,
)
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import StandardScaler

from config import (
    FEATURED_HONGGUANG, FEATURED_KAZUO,
    META_COLS, LABEL_COLS, CUMULATIVE_LABEL_COLS,
    CV_FOLDS, SHAP_TOP_N, RANDOM_SEED,
)

warnings.filterwarnings("ignore", category=UserWarning)

# ============================================================
# è·¯å¾„ä¸å¸¸é‡
# ============================================================
SCRIPT_DIR = Path(__file__).resolve().parent
OUTPUT_DIR = SCRIPT_DIR / "output_v1.1"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# v1.0 è¾“å‡ºï¼ˆç”¨äºå¯¹æ¯”ï¼‰
V1_OUTPUT_DIR = SCRIPT_DIR / "output"

# PINN å­¦ä¹ åˆ°çš„ç‰©ç†å‚æ•°ï¼ˆä¸¤ç«™å¹³å‡å€¼ï¼Œè·¨åŸŸä¸å˜å‡è®¾ï¼‰
PINN_PARAMS = {
    "K_La": (1.3497 + 1.3666) / 2,         # å¤æ°§ä¼ è´¨ç³»æ•° (day^-1)
    "R_fish_base": (1.5687 + 1.4456) / 2,   # é±¼åŸºç¡€è€—æ°§ç‡ (mg/L/day)
    "alpha_T": (0.0341 + 0.0335) / 2,       # è€—æ°§æ¸©åº¦ç³»æ•°
    "R_bio": (0.7769 + 0.7251) / 2,         # å¾®ç”Ÿç‰©è€—æ°§ç‡ (mg/L/day)
    "P_photo_rate": (0.0678 + 0.0709) / 2,  # å…‰åˆäº§æ°§é€Ÿç‡
    "T_ref": 25.0,                           # å‚è€ƒæ¸©åº¦
}

# é¢„æµ‹ä»»åŠ¡å®šä¹‰
TASKS = {
    "è”¬èœç—…å®³": {
        "label_col": "è”¬èœ_ç—…å®³æ¬¡æ•°",
        "description": "è”¬èœæ˜¯å¦å‘ç”Ÿç—…å®³ï¼ˆäºŒåˆ†ç±»ï¼‰",
    },
    "é±¼ç±»æ­»äº¡": {
        "label_col": "é±¼_æ­»äº¡æ•°é‡",
        "description": "é±¼ç±»æ˜¯å¦å‘ç”Ÿæ­»äº¡ï¼ˆäºŒåˆ†ç±»ï¼‰",
    },
}

# v1.1 æ›´å¼ºæ­£åˆ™åŒ–çš„ XGBoost å‚æ•°
XGBOOST_PARAMS_V11 = {
    "n_estimators": 500,
    "max_depth": 4,              # 6â†’4 é™ä½è¿‡æ‹Ÿåˆ
    "learning_rate": 0.03,       # 0.05â†’0.03 æ›´å°æ­¥é•¿
    "subsample": 0.7,            # 0.8â†’0.7
    "colsample_bytree": 0.7,     # 0.8â†’0.7
    "min_child_weight": 10,      # 5â†’10 æ›´å¼ºçº¦æŸ
    "gamma": 0.3,                # 0.1â†’0.3
    "reg_alpha": 0.5,            # 0.1â†’0.5 L1 æ­£åˆ™
    "reg_lambda": 3.0,           # 1.0â†’3.0 L2 æ­£åˆ™
    "random_state": RANDOM_SEED,
    "n_jobs": -1,
    "eval_metric": "logloss",
    "early_stopping_rounds": 50,
}

# åŸŸåç§»é£é™©é«˜çš„ç‰¹å¾ï¼ˆè·¨åŸºåœ°è¯­ä¹‰ä¸ä¸€è‡´ï¼‰
DOMAIN_SHIFT_FEATURES = [
    "èƒ½è€—km/h",                   # ä¸åŒåŸºåœ°è®¾å¤‡ä½“ç³»ä¸åŒï¼Œå«ä¹‰ä¸åŒ
    "ç§æ¤åºŠ1æ¶²ä½ä¸Šé™è·ç§æ¤åºŠè¡¨é¢è·ç¦»cm",  # è®¾å¤‡å·®å¼‚
    "ç§æ¤åºŠ2æ¶²ä½ä¸Šé™è·ç§æ¤åºŠè¡¨é¢è·ç¦»cm",
]


# ============================================================
# ä¸­æ–‡å­—ä½“è®¾ç½®
# ============================================================
def setup_chinese_font():
    font_candidates = [
        "/System/Library/Fonts/STHeiti Light.ttc",
        "/System/Library/Fonts/PingFang.ttc",
        "/System/Library/Fonts/Supplemental/Songti.ttc",
        "/System/Library/Fonts/Hiragino Sans GB.ttc",
    ]
    for fp in font_candidates:
        try:
            fm.fontManager.addfont(fp)
            prop = fm.FontProperties(fname=fp)
            plt.rcParams["font.family"] = prop.get_name()
            plt.rcParams["axes.unicode_minus"] = False
            print(f"âœ… ä½¿ç”¨ä¸­æ–‡å­—ä½“: {prop.get_name()}")
            return
        except Exception:
            continue
    plt.rcParams["font.sans-serif"] = ["Arial Unicode MS", "SimHei", "DejaVu Sans"]
    plt.rcParams["axes.unicode_minus"] = False

setup_chinese_font()


# ============================================================
# ç‰©ç†ç‰¹å¾è®¡ç®—ï¼ˆåŸºäº PINN å­¦åˆ°çš„å‚æ•°ï¼‰
# ============================================================
def do_saturation(T):
    """é¥±å’Œæº¶æ°§æµ“åº¦ (Benson & Krause, 1984 ç®€åŒ–ç‰ˆ)"""
    return 14.62 - 0.3898 * T + 0.006969 * T**2 - 5.897e-5 * T**3


def compute_physics_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    åŸºäº PINN å­¦åˆ°çš„ç‰©ç†æ–¹ç¨‹ï¼Œè®¡ç®—è·¨åŸºåœ°ä¸å˜çš„ç‰©ç†ç‰¹å¾ã€‚

    æ–°å¢ç‰¹å¾ï¼š
      - pinn_DO_deficit:    DO_sat(T) - DO_actualï¼Œæº¶æ°§äºæŸ
      - pinn_R_fish_T:      æ¸©åº¦ä¿®æ­£åçš„é±¼è€—æ°§ç‡
      - pinn_reaeration:    å¤æ°§é€Ÿç‡ K_La * (DO_sat - DO)
      - pinn_P_photo:       å…‰åˆäº§æ°§é‡
      - pinn_oxygen_stress: æ°§æ°”å‡€æ¶ˆè€—ï¼ˆæ­£å€¼=ç¼ºæ°§é£é™©ï¼‰
      - pinn_DO_margin:     è· 2mg/L è­¦æˆ’çº¿çš„ä½™é‡
      - pinn_DO_sat_ratio:  å®é™…DO / é¥±å’ŒDOï¼ˆç‰©ç†å½’ä¸€åŒ–ï¼‰
    """
    p = PINN_PARAMS
    T_water = df["æ°´æ¸©_æ—¥å‡"].fillna(25.0)
    DO_actual = df["æº¶æ°§mg/L"].fillna(df["æº¶æ°§mg/L"].median())
    light_h = df["å…‰ç…§æ—¶é•¿h"].fillna(0.0)

    DO_sat = do_saturation(T_water)

    out = pd.DataFrame(index=df.index)

    # 1. æº¶æ°§äºæŸï¼ˆDO_sat - DO_actualï¼‰
    out["pinn_DO_deficit"] = DO_sat - DO_actual

    # 2. æ¸©åº¦ä¿®æ­£åé±¼è€—æ°§ç‡
    out["pinn_R_fish_T"] = p["R_fish_base"] * (1.0 + p["alpha_T"] * (T_water - p["T_ref"]))

    # 3. å¤æ°§é€Ÿç‡
    out["pinn_reaeration"] = p["K_La"] * (DO_sat - DO_actual)

    # 4. å…‰åˆäº§æ°§
    out["pinn_P_photo"] = p["P_photo_rate"] * light_h

    # 5. æ°§æ°”å‡€å‹åŠ›æŒ‡æ•°ï¼ˆæ­£ = ç¼ºæ°§é£é™©ï¼Œè´Ÿ = å®‰å…¨ï¼‰
    out["pinn_oxygen_stress"] = (
        out["pinn_R_fish_T"] + p["R_bio"]
        - out["pinn_reaeration"]
        - out["pinn_P_photo"]
    )

    # 6. è·è­¦æˆ’çº¿ä½™é‡
    out["pinn_DO_margin"] = DO_actual - 2.0

    # 7. ç‰©ç†å½’ä¸€åŒ– DOï¼ˆæ— é‡çº²ï¼Œå¯è·¨åŸºåœ°æ¯”è¾ƒï¼‰
    out["pinn_DO_sat_ratio"] = DO_actual / DO_sat.clip(lower=1.0)

    return out


# ============================================================
# æ—¶åºè¶‹åŠ¿ç‰¹å¾
# ============================================================
def compute_temporal_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    å¢å¼ºæ—¶åºè¶‹åŠ¿ç‰¹å¾ï¼ˆè¡¥å……è¶…å‡ºåŸå§‹ç‰¹å¾å·¥ç¨‹çš„éƒ¨åˆ†ï¼‰ã€‚

    æ–°å¢ï¼š
      - æ°´æ¸©çŸ­æœŸè¶‹åŠ¿ï¼ˆå½“å‰ - 3æ—¥å‡å€¼ï¼‰
      - æº¶æ°§çŸ­æœŸå˜åŒ–å¹…åº¦
      - æ°´æ°”æ¸©äº¤äº’é¡¹
      - ç´¯ç§¯é«˜æ¸©å¤©æ•°ï¼ˆ7æ—¥å†…æ°´æ¸©>28â„ƒï¼‰
      - ç´¯ç§¯ä½æ°§å¤©æ•°ï¼ˆ7æ—¥å†…æº¶æ°§<5ï¼‰
      - æ¸©åº¦æ³¢åŠ¨åŠ é€Ÿåº¦ï¼ˆ3æ—¥stdçš„å˜åŒ–ç‡ï¼‰
    """
    out = pd.DataFrame(index=df.index)

    T_water = df["æ°´æ¸©_æ—¥å‡"].fillna(25.0)
    T_air = df["æ°”æ¸©_æ—¥å‡"].fillna(20.0)
    DO = df["æº¶æ°§mg/L"].fillna(df["æº¶æ°§mg/L"].median())

    # çŸ­æœŸè¶‹åŠ¿
    roll3_water = df.get("æ°´æ¸©_æ—¥å‡_roll3d_mean")
    if roll3_water is not None:
        out["trend_æ°´æ¸©_3d"] = T_water - roll3_water.fillna(T_water)
    else:
        out["trend_æ°´æ¸©_3d"] = 0.0

    # æº¶æ°§å˜åŒ–å¹…åº¦ï¼ˆç”¨å˜åŒ–ç‡çš„ç»å¯¹å€¼è¡¡é‡æ³¢åŠ¨ï¼‰
    do_change = df.get("æº¶æ°§_å˜åŒ–ç‡")
    if do_change is not None:
        out["DO_volatility"] = do_change.fillna(0).abs()
    else:
        out["DO_volatility"] = 0.0

    # äº¤äº’é¡¹ï¼šæ°´æ¸© Ã— æ°”æ¸©ï¼ˆåŒé«˜æ¸©è€¦åˆé£é™©ï¼‰
    out["æ°´æ°”æ¸©_äº¤äº’"] = T_water * T_air / 100.0  # å½’ä¸€åŒ–

    # æ°´æ¸©åç¦»é€‚å®œèŒƒå›´ï¼ˆ18~28â„ƒï¼‰
    out["æ°´æ¸©_åç¦»é€‚å®œ"] = np.where(
        T_water > 28, T_water - 28,
        np.where(T_water < 18, 18 - T_water, 0)
    )

    # 7æ—¥ç´¯ç§¯é«˜æ¸©å¤©æ•°ï¼ˆæ°´æ¸© > 28â„ƒï¼‰
    high_temp = (T_water > 28).astype(float)
    out["é«˜æ¸©å¤©æ•°_7d"] = high_temp.rolling(7, min_periods=1).sum().values

    # 7æ—¥ç´¯ç§¯ä½æ°§å¤©æ•°ï¼ˆæº¶æ°§ < 5 mg/Lï¼‰
    low_do_flag = (DO < 5).astype(float)
    out["ä½æ°§å¤©æ•°_7d"] = low_do_flag.rolling(7, min_periods=1).sum().values

    # æ¸©åº¦æ³¢åŠ¨åŠ é€Ÿåº¦
    roll3_std = df.get("æ°´æ¸©_æ—¥å‡_roll3d_std")
    if roll3_std is not None:
        out["æ°´æ¸©æ³¢åŠ¨_åŠ é€Ÿåº¦"] = roll3_std.fillna(0).diff().fillna(0)
    else:
        out["æ°´æ¸©æ³¢åŠ¨_åŠ é€Ÿåº¦"] = 0.0

    return out


# ============================================================
# åŸŸè‡ªé€‚åº”ï¼šå®ä¾‹åŠ æƒ
# ============================================================
def compute_domain_weights(X_source: pd.DataFrame, X_target: pd.DataFrame,
                            method: str = "density_ratio") -> np.ndarray:
    """
    åŸŸè‡ªé€‚åº”å®ä¾‹åŠ æƒï¼šä¸Šè°ƒä¸ç›®æ ‡åŸŸï¼ˆå–€å·¦ï¼‰åˆ†å¸ƒç›¸ä¼¼çš„æºåŸŸï¼ˆçº¢å…‰ï¼‰æ ·æœ¬æƒé‡ã€‚

    æ–¹æ³•ï¼šåŸºäºç‰¹å¾è·ç¦»çš„å¯†åº¦æ¯”ä¼°è®¡ï¼ˆKernel Mean Matching ç®€åŒ–ç‰ˆï¼‰ã€‚
    ä½¿ç”¨ PCA é™ç»´åè®¡ç®—æ¯ä¸ªæºåŸŸæ ·æœ¬åˆ°ç›®æ ‡åŸŸè´¨å¿ƒçš„é©¬æ°è·ç¦»ã€‚

    Returns:
        weights: (n_source,) æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æƒé‡
    """
    from sklearn.decomposition import PCA

    # é€‰æ‹©æ•°å€¼åˆ—
    common_cols = [c for c in X_source.columns if c in X_target.columns]

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    src_scaled = scaler.fit_transform(X_source[common_cols].fillna(0))
    tgt_scaled = scaler.transform(X_target[common_cols].fillna(0))

    # PCA é™åˆ° 10 ç»´
    n_comp = min(10, src_scaled.shape[1])
    pca = PCA(n_components=n_comp, random_state=RANDOM_SEED)
    src_pca = pca.fit_transform(src_scaled)
    tgt_pca = pca.transform(tgt_scaled)

    # ç›®æ ‡åŸŸè´¨å¿ƒ
    tgt_centroid = tgt_pca.mean(axis=0)

    # æ¯ä¸ªæºåŸŸæ ·æœ¬åˆ°ç›®æ ‡åŸŸè´¨å¿ƒçš„æ¬§æ°è·ç¦»
    distances = np.sqrt(((src_pca - tgt_centroid) ** 2).sum(axis=1))

    # è·ç¦» â†’ æƒé‡ï¼ˆé«˜æ–¯æ ¸ï¼šè·ç¦»è¶Šè¿‘æƒé‡è¶Šé«˜ï¼‰
    sigma = np.median(distances) + 1e-8
    weights = np.exp(-0.5 * (distances / sigma) ** 2)

    # å½’ä¸€åŒ–ä½¿å¾—å‡å€¼ä¸º 1ï¼ˆä¸æ”¹å˜æ€»æ ·æœ¬æƒé‡ï¼‰
    weights = weights / weights.mean()

    # è£å‰ªæç«¯å€¼
    weights = np.clip(weights, 0.1, 5.0)

    print(f"  ğŸ¯ åŸŸè‡ªé€‚åº”æƒé‡: min={weights.min():.3f}, max={weights.max():.3f}, "
          f"mean={weights.mean():.3f}, std={weights.std():.3f}")

    return weights


# ============================================================
# æ•°æ®åŠ è½½ + ç‰¹å¾å¢å¼º
# ============================================================
def load_and_enhance(task_config: dict, remove_domain_shift: bool = True):
    """
    åŠ è½½æ•°æ®å¹¶å¢åŠ ç‰©ç† + æ—¶åºç‰¹å¾ã€‚

    Returns:
        X_train, X_val, y_train, y_val: çº¢å…‰è®­ç»ƒ/éªŒè¯
        X_test, y_test: å–€å·¦ç‹¬ç«‹æµ‹è¯•
        feature_names: ç‰¹å¾åˆ—è¡¨
        domain_weights: è®­ç»ƒé›†åŸŸè‡ªé€‚åº”æƒé‡
    """
    label_col = task_config["label_col"]

    # åŠ è½½åŸå§‹ featured æ•°æ®
    df_hg = pd.read_csv(FEATURED_HONGGUANG, parse_dates=["æ—¥æœŸ"])
    df_kz = pd.read_csv(FEATURED_KAZUO, parse_dates=["æ—¥æœŸ"])
    print(f"[çº¢å…‰] åŠ è½½: {df_hg.shape[0]} è¡Œ, {df_hg.shape[1]} åˆ—")
    print(f"[å–€å·¦] åŠ è½½: {df_kz.shape[0]} è¡Œ, {df_kz.shape[1]} åˆ—")

    # ====== å¢åŠ ç‰©ç†ç‰¹å¾ ======
    physics_hg = compute_physics_features(df_hg)
    physics_kz = compute_physics_features(df_kz)
    df_hg = pd.concat([df_hg, physics_hg], axis=1)
    df_kz = pd.concat([df_kz, physics_kz], axis=1)
    print(f"  âœ… æ–°å¢ {physics_hg.shape[1]} ä¸ª PINN ç‰©ç†ç‰¹å¾")

    # ====== å¢åŠ æ—¶åºè¶‹åŠ¿ç‰¹å¾ ======
    temporal_hg = compute_temporal_features(df_hg)
    temporal_kz = compute_temporal_features(df_kz)
    df_hg = pd.concat([df_hg, temporal_hg], axis=1)
    df_kz = pd.concat([df_kz, temporal_kz], axis=1)
    print(f"  âœ… æ–°å¢ {temporal_hg.shape[1]} ä¸ªæ—¶åºè¶‹åŠ¿ç‰¹å¾")

    # ====== æ’é™¤éç‰¹å¾åˆ— ======
    exclude = set(META_COLS + LABEL_COLS + CUMULATIVE_LABEL_COLS)
    if remove_domain_shift:
        exclude.update(DOMAIN_SHIFT_FEATURES)
        print(f"  âš ï¸ ç§»é™¤ {len(DOMAIN_SHIFT_FEATURES)} ä¸ªé«˜åŸŸåç§»ç‰¹å¾: {DOMAIN_SHIFT_FEATURES}")

    feature_cols = [c for c in df_hg.columns if c not in exclude]
    X_hg = df_hg[feature_cols].copy()
    y_hg = (df_hg[label_col] > 0).astype(int)

    # ç¡®ä¿å–€å·¦æœ‰ç›¸åŒç‰¹å¾
    for c in feature_cols:
        if c not in df_kz.columns:
            df_kz[c] = 0
    X_kz = df_kz[feature_cols].copy()
    y_kz = (df_kz[label_col] > 0).astype(int)

    # ç¼ºå¤±å€¼å¤„ç†
    for X in [X_hg, X_kz]:
        for col in X.columns:
            if X[col].isnull().any():
                X[col] = X[col].fillna(X[col].median())
        X.replace([np.inf, -np.inf], np.nan, inplace=True)
        for col in X.columns:
            if X[col].isnull().any():
                X[col] = X[col].fillna(0)

    feature_names = list(X_hg.columns)

    print(f"  ç‰¹å¾æ•°: {len(feature_names)}")
    print(f"  çº¢å…‰æ­£æ ·æœ¬: {y_hg.sum()} ({y_hg.mean()*100:.1f}%)")
    print(f"  å–€å·¦æ­£æ ·æœ¬: {y_kz.sum()} ({y_kz.mean()*100:.1f}%)")

    # æ‹†åˆ†è®­ç»ƒ/éªŒè¯
    X_train, X_val, y_train, y_val = train_test_split(
        X_hg, y_hg, test_size=0.2, random_state=RANDOM_SEED, stratify=y_hg
    )

    # ====== åŸŸè‡ªé€‚åº”æƒé‡ ======
    print("\nğŸ¯ è®¡ç®—åŸŸè‡ªé€‚åº”å®ä¾‹æƒé‡...")
    domain_weights = compute_domain_weights(X_train, X_kz)

    return X_train, X_val, y_train, y_val, X_kz, y_kz, feature_names, domain_weights


# ============================================================
# è®­ç»ƒ
# ============================================================
def train_xgboost_v11(X_train, y_train, X_val, y_val, sample_weight,
                       task_name: str):
    """è®­ç»ƒå¸¦åŸŸè‡ªé€‚åº”æƒé‡çš„ XGBoost"""
    n_pos = y_train.sum()
    n_neg = len(y_train) - n_pos
    scale_pos_weight = n_neg / max(n_pos, 1)
    print(f"\nğŸ”§ class imbalance â€” scale_pos_weight = {scale_pos_weight:.2f}")

    params = XGBOOST_PARAMS_V11.copy()
    early_stop = params.pop("early_stopping_rounds", 50)

    model = xgb.XGBClassifier(
        scale_pos_weight=scale_pos_weight,
        use_label_encoder=False,
        **params,
    )

    model.fit(
        X_train, y_train,
        sample_weight=sample_weight,
        eval_set=[(X_val, y_val)],
        verbose=50,
    )

    model_path = OUTPUT_DIR / f"xgb_{task_name}_v1.1.json"
    model.save_model(str(model_path))
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    return model


# ============================================================
# é˜ˆå€¼ä¼˜åŒ–
# ============================================================
def find_optimal_threshold(y_true, y_prob):
    """åŸºäº F1 æœ€å¤§åŒ–çš„é˜ˆå€¼æœç´¢"""
    best_f1, best_thr = 0, 0.5
    for thr in np.arange(0.1, 0.9, 0.01):
        y_pred = (y_prob >= thr).astype(int)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        if f1 > best_f1:
            best_f1 = f1
            best_thr = thr
    return best_thr, best_f1


# ============================================================
# è¯„ä¼°
# ============================================================
def evaluate(model, X, y, dataset_name: str, task_name: str, threshold=None):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæ”¯æŒè‡ªå®šä¹‰é˜ˆå€¼"""
    y_prob = model.predict_proba(X)[:, 1]

    if threshold is None:
        y_pred = model.predict(X)
        used_threshold = 0.5
    else:
        y_pred = (y_prob >= threshold).astype(int)
        used_threshold = threshold

    metrics = {
        "æ•°æ®é›†": dataset_name,
        "ä»»åŠ¡": task_name,
        "é˜ˆå€¼": used_threshold,
        "æ ·æœ¬æ•°": len(y),
        "æ­£æ ·æœ¬æ•°": int(y.sum()),
        "æ­£æ ·æœ¬æ¯”ä¾‹": f"{y.mean()*100:.1f}%",
        "Accuracy": accuracy_score(y, y_pred),
        "Precision": precision_score(y, y_pred, zero_division=0),
        "Recall": recall_score(y, y_pred, zero_division=0),
        "F1-Score": f1_score(y, y_pred, zero_division=0),
        "ROC-AUC": roc_auc_score(y, y_prob) if y.nunique() > 1 else 0,
        "AP": average_precision_score(y, y_prob) if y.nunique() > 1 else 0,
    }

    print(f"\n{'â”€'*55}")
    print(f"ğŸ“ˆ è¯„ä¼°: {dataset_name} ({task_name}) | é˜ˆå€¼={used_threshold:.2f}")
    print(f"{'â”€'*55}")
    for k, v in metrics.items():
        if isinstance(v, float):
            print(f"  {k:25s}: {v:.4f}")
        else:
            print(f"  {k:25s}: {v}")

    cm = confusion_matrix(y, y_pred)
    print(f"\n  æ··æ·†çŸ©é˜µ:")
    print(f"  TN={cm[0,0]:6d}  FP={cm[0,1]:6d}")
    print(f"  FN={cm[1,0]:6d}  TP={cm[1,1]:6d}")

    # === å¯è§†åŒ– ===
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle(f"[v1.1] {task_name} â€” {dataset_name}", fontsize=14, fontweight="bold")

    # æ··æ·†çŸ©é˜µ
    ax = axes[0]
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax,
                xticklabels=["æ— ", "æœ‰"], yticklabels=["æ— ", "æœ‰"])
    ax.set_xlabel("é¢„æµ‹")
    ax.set_ylabel("å®é™…")
    ax.set_title("æ··æ·†çŸ©é˜µ")

    # ROC
    ax = axes[1]
    if y.nunique() > 1:
        fpr, tpr, _ = roc_curve(y, y_prob)
        ax.plot(fpr, tpr, "b-", lw=2, label=f"AUC = {metrics['ROC-AUC']:.4f}")
        ax.plot([0, 1], [0, 1], "k--", alpha=0.3)
        ax.set_xlabel("FPR")
        ax.set_ylabel("TPR")
        ax.set_title("ROC æ›²çº¿")
        ax.legend()

    # PR
    ax = axes[2]
    if y.nunique() > 1:
        prec, rec, _ = precision_recall_curve(y, y_prob)
        ax.plot(rec, prec, "r-", lw=2, label=f"AP = {metrics['AP']:.4f}")
        ax.axhline(y=y.mean(), color="gray", ls="--", alpha=0.5, label="éšæœºåŸºçº¿")
        ax.set_xlabel("Recall")
        ax.set_ylabel("Precision")
        ax.set_title("PR æ›²çº¿")
        ax.legend()

    plt.tight_layout()
    fig_path = OUTPUT_DIR / f"eval_{task_name}_{dataset_name}.png"
    plt.savefig(fig_path, dpi=150, bbox_inches="tight")
    plt.close()
    print(f"ğŸ“Š å›¾è¡¨: {fig_path}")

    return metrics


# ============================================================
# äº¤å‰éªŒè¯
# ============================================================
def cross_validate(X, y, sample_weight, task_name):
    """5æŠ˜äº¤å‰éªŒè¯"""
    print(f"\n{'='*50}")
    print(f"ğŸ”„ {CV_FOLDS}æŠ˜äº¤å‰éªŒè¯ â€” {task_name} (v1.1)")
    print(f"{'='*50}")

    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_SEED)
    fold_metrics = []

    n_pos = y.sum()
    n_neg = len(y) - n_pos
    scale_pos_weight = n_neg / max(n_pos, 1)

    params = XGBOOST_PARAMS_V11.copy()
    params.pop("early_stopping_rounds", None)

    for fold_i, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]
        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]
        w_tr = sample_weight[train_idx] if sample_weight is not None else None

        model = xgb.XGBClassifier(
            scale_pos_weight=scale_pos_weight,
            use_label_encoder=False,
            **params,
        )
        model.fit(X_tr, y_tr, sample_weight=w_tr, verbose=0)

        y_pred = model.predict(X_vl)
        y_prob = model.predict_proba(X_vl)[:, 1]

        fold_metrics.append({
            "Fold": fold_i,
            "F1": f1_score(y_vl, y_pred, zero_division=0),
            "Precision": precision_score(y_vl, y_pred, zero_division=0),
            "Recall": recall_score(y_vl, y_pred, zero_division=0),
            "AUC": roc_auc_score(y_vl, y_prob) if y_vl.nunique() > 1 else 0,
        })
        print(f"  Fold {fold_i}: F1={fold_metrics[-1]['F1']:.4f}, "
              f"AUC={fold_metrics[-1]['AUC']:.4f}")

    df_cv = pd.DataFrame(fold_metrics)
    print(f"\n  å¹³å‡ F1:  {df_cv['F1'].mean():.4f} Â± {df_cv['F1'].std():.4f}")
    print(f"  å¹³å‡ AUC: {df_cv['AUC'].mean():.4f} Â± {df_cv['AUC'].std():.4f}")
    return df_cv


# ============================================================
# SHAP åˆ†æ
# ============================================================
def shap_analysis(model, X, feature_names, task_name, dataset_name=""):
    """SHAP å¯è§£é‡Šæ€§åˆ†æ"""
    print(f"\n{'='*50}")
    print(f"ğŸ” SHAP åˆ†æ â€” {task_name} ({dataset_name}) [v1.1]")
    print(f"{'='*50}")

    explainer = shap.TreeExplainer(model)

    if len(X) > 5000:
        X_sample = X.sample(5000, random_state=RANDOM_SEED)
    else:
        X_sample = X

    shap_values = explainer.shap_values(X_sample)

    # Summary plot
    fig, ax = plt.subplots(figsize=(12, 8))
    shap.summary_plot(
        shap_values, X_sample, feature_names=feature_names,
        max_display=SHAP_TOP_N, show=False,
    )
    plt.title(f"[v1.1] {task_name} â€” SHAP (Top {SHAP_TOP_N})", fontsize=14)
    plt.tight_layout()
    fig_path = OUTPUT_DIR / f"shap_summary_{task_name}_{dataset_name}.png"
    plt.savefig(fig_path, dpi=150, bbox_inches="tight")
    plt.close()

    # Bar plot
    fig, ax = plt.subplots(figsize=(10, 8))
    shap.summary_plot(
        shap_values, X_sample, feature_names=feature_names,
        plot_type="bar", max_display=SHAP_TOP_N, show=False,
    )
    plt.title(f"[v1.1] {task_name} â€” SHAP å‡å€¼ (Top {SHAP_TOP_N})", fontsize=14)
    plt.tight_layout()
    fig_path = OUTPUT_DIR / f"shap_bar_{task_name}_{dataset_name}.png"
    plt.savefig(fig_path, dpi=150, bbox_inches="tight")
    plt.close()

    # Importance ranking
    mean_abs_shap = np.abs(shap_values).mean(axis=0)
    importance_df = pd.DataFrame({
        "ç‰¹å¾": feature_names,
        "SHAPå‡å€¼": mean_abs_shap,
    }).sort_values("SHAPå‡å€¼", ascending=False).reset_index(drop=True)
    importance_df.index += 1
    importance_df.index.name = "æ’å"

    csv_path = OUTPUT_DIR / f"shap_importance_{task_name}_{dataset_name}.csv"
    importance_df.to_csv(csv_path)

    # æ ‡è®°ç‰©ç†/æ—¶åºç‰¹å¾
    pinn_feats = [f for f in importance_df["ç‰¹å¾"] if f.startswith("pinn_")]
    temporal_feats = [f for f in importance_df["ç‰¹å¾"]
                      if f.startswith("trend_") or f in ["DO_volatility", "æ°´æ°”æ¸©_äº¤äº’",
                         "æ°´æ¸©_åç¦»é€‚å®œ", "é«˜æ¸©å¤©æ•°_7d", "ä½æ°§å¤©æ•°_7d", "æ°´æ¸©æ³¢åŠ¨_åŠ é€Ÿåº¦"]]

    pinn_ranks = importance_df[importance_df["ç‰¹å¾"].isin(pinn_feats)]
    temporal_ranks = importance_df[importance_df["ç‰¹å¾"].isin(temporal_feats)]

    print(f"\n  ğŸ“‹ Top {SHAP_TOP_N} ç‰¹å¾:")
    print(importance_df.head(SHAP_TOP_N).to_string())
    print(f"\n  ğŸ”¬ PINN ç‰©ç†ç‰¹å¾æ’å:")
    print(pinn_ranks.to_string() if len(pinn_ranks) > 0 else "    (æ— )")
    print(f"\n  ğŸ“ˆ æ—¶åºè¶‹åŠ¿ç‰¹å¾æ’å:")
    print(temporal_ranks.to_string() if len(temporal_ranks) > 0 else "    (æ— )")

    return importance_df


# ============================================================
# v1.0 vs v1.1 å¯¹æ¯”
# ============================================================
def load_v1_report(task_name):
    """åŠ è½½ v1.0 çš„ç»“æœç”¨äºå¯¹æ¯”"""
    report_path = V1_OUTPUT_DIR / f"report_{task_name}.json"
    if report_path.exists():
        with open(report_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return None


def generate_comparison(task_name, v11_metrics_val, v11_metrics_test):
    """ç”Ÿæˆ v1.0 vs v1.1 å¯¹æ¯”æŠ¥å‘Š"""
    v1 = load_v1_report(task_name)
    if v1 is None:
        return None

    comparison = {
        "ä»»åŠ¡": task_name,
        "éªŒè¯é›†_çº¢å…‰": {
            "v1.0_AUC": v1.get("éªŒè¯é›†", {}).get("ROC-AUC"),
            "v1.1_AUC": v11_metrics_val["ROC-AUC"],
            "v1.0_F1": v1.get("éªŒè¯é›†", {}).get("F1-Score"),
            "v1.1_F1": v11_metrics_val["F1-Score"],
            "AUC_å˜åŒ–": f"{(v11_metrics_val['ROC-AUC'] - v1['éªŒè¯é›†']['ROC-AUC'])*100:+.2f}%",
            "F1_å˜åŒ–": f"{(v11_metrics_val['F1-Score'] - v1['éªŒè¯é›†']['F1-Score'])*100:+.2f}%",
        },
        "æµ‹è¯•é›†_å–€å·¦": {
            "v1.0_AUC": v1.get("ç‹¬ç«‹æµ‹è¯•é›†_å–€å·¦", {}).get("ROC-AUC"),
            "v1.1_AUC": v11_metrics_test["ROC-AUC"],
            "v1.0_F1": v1.get("ç‹¬ç«‹æµ‹è¯•é›†_å–€å·¦", {}).get("F1-Score"),
            "v1.1_F1": v11_metrics_test["F1-Score"],
            "AUC_å˜åŒ–": f"{(v11_metrics_test['ROC-AUC'] - v1['ç‹¬ç«‹æµ‹è¯•é›†_å–€å·¦']['ROC-AUC'])*100:+.2f}%",
            "F1_å˜åŒ–": f"{(v11_metrics_test['F1-Score'] - v1['ç‹¬ç«‹æµ‹è¯•é›†_å–€å·¦']['F1-Score'])*100:+.2f}%",
        },
    }

    print(f"\n{'='*60}")
    print(f"ğŸ“Š v1.0 vs v1.1 å¯¹æ¯” â€” {task_name}")
    print(f"{'='*60}")
    for split_name, data in [("éªŒè¯é›†_çº¢å…‰", comparison["éªŒè¯é›†_çº¢å…‰"]),
                              ("æµ‹è¯•é›†_å–€å·¦", comparison["æµ‹è¯•é›†_å–€å·¦"])]:
        print(f"\n  {split_name}:")
        print(f"    AUC: {data['v1.0_AUC']:.4f} â†’ {data['v1.1_AUC']:.4f} ({data['AUC_å˜åŒ–']})")
        print(f"    F1:  {data['v1.0_F1']:.4f} â†’ {data['v1.1_F1']:.4f} ({data['F1_å˜åŒ–']})")

    return comparison


# ============================================================
# ä¸»æµç¨‹
# ============================================================
def run_task(task_name: str, task_config: dict):
    """è¿è¡Œ v1.1 é¢„æµ‹ä»»åŠ¡"""
    print(f"\n{'#'*60}")
    print(f"## [v1.1] {task_name} â€” {task_config['description']}")
    print(f"{'#'*60}")

    # 1. åŠ è½½æ•°æ® + ç‰¹å¾å¢å¼º
    X_train, X_val, y_train, y_val, X_test, y_test, feature_names, domain_weights = \
        load_and_enhance(task_config, remove_domain_shift=True)

    # 2. äº¤å‰éªŒè¯
    X_all_hg = pd.concat([X_train, X_val], axis=0)
    y_all_hg = pd.concat([y_train, y_val], axis=0)
    # åˆå¹¶æƒé‡
    all_weights = np.concatenate([
        domain_weights,
        compute_domain_weights(X_val, X_test)
    ])
    cv_results = cross_validate(X_all_hg, y_all_hg, all_weights, task_name)

    # 3. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print(f"\nğŸš€ è®­ç»ƒ v1.1 æ¨¡å‹...")
    model = train_xgboost_v11(X_train, y_train, X_val, y_val,
                               domain_weights, task_name)

    # 4. åœ¨éªŒè¯é›†ä¸Šæ‰¾æœ€ä¼˜é˜ˆå€¼
    y_val_prob = model.predict_proba(X_val)[:, 1]
    opt_thr, opt_f1 = find_optimal_threshold(y_val, y_val_prob)
    print(f"\nğŸ¯ æœ€ä¼˜é˜ˆå€¼: {opt_thr:.2f} (éªŒè¯é›† F1={opt_f1:.4f})")

    # 5. è¯„ä¼°
    val_metrics = evaluate(model, X_val, y_val, "éªŒè¯é›†_çº¢å…‰", task_name, threshold=opt_thr)
    test_metrics = evaluate(model, X_test, y_test, "æµ‹è¯•é›†_å–€å·¦", task_name, threshold=opt_thr)

    # 6. SHAP åˆ†æ
    importance = shap_analysis(model, X_val, feature_names, task_name, "éªŒè¯é›†")

    # 7. v1.0 vs v1.1 å¯¹æ¯”
    comparison = generate_comparison(task_name, val_metrics, test_metrics)

    # 8. æ±‡æ€»æŠ¥å‘Š
    report = {
        "ç‰ˆæœ¬": "v1.1",
        "ä»»åŠ¡": task_name,
        "æè¿°": task_config["description"],
        "æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "æ”¹è¿›ç‚¹": [
            "PINN ç‰©ç†ç‰¹å¾æ³¨å…¥ï¼ˆ7ä¸ªæ–°ç‰¹å¾ï¼‰",
            "æ—¶åºè¶‹åŠ¿ç‰¹å¾å¢å¼ºï¼ˆ7ä¸ªæ–°ç‰¹å¾ï¼‰",
            "åŸŸè‡ªé€‚åº”å®ä¾‹åŠ æƒ",
            "ç§»é™¤é«˜åŸŸåç§»ç‰¹å¾",
            "æ›´å¼ºæ­£åˆ™åŒ–ï¼ˆmax_depth=4, L1/L2å¢å¼ºï¼‰",
            f"F1-æœ€ä¼˜é˜ˆå€¼: {opt_thr:.2f}",
        ],
        "PINNå‚æ•°": PINN_PARAMS,
        "äº¤å‰éªŒè¯": {
            "F1_mean": float(cv_results["F1"].mean()),
            "F1_std": float(cv_results["F1"].std()),
            "AUC_mean": float(cv_results["AUC"].mean()),
            "AUC_std": float(cv_results["AUC"].std()),
        },
        "éªŒè¯é›†": {k: v for k, v in val_metrics.items()
                    if isinstance(v, (int, float, str))},
        "ç‹¬ç«‹æµ‹è¯•é›†_å–€å·¦": {k: v for k, v in test_metrics.items()
                           if isinstance(v, (int, float, str))},
        "Top10ç‰¹å¾": importance.head(10)["ç‰¹å¾"].tolist(),
        "vs_v1.0": comparison,
    }

    report_path = OUTPUT_DIR / f"report_{task_name}_v1.1.json"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    print(f"\nğŸ“„ æŠ¥å‘Š: {report_path}")

    return model, report


def main():
    print("=" * 70)
    print("  ğŸŸğŸ¥¬ é±¼èœå…±ç”Ÿç—…å®³é¢„æµ‹ v1.1 â€” PINN + æ—¶åº + åŸŸè‡ªé€‚åº”")
    print("=" * 70)
    print(f"  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"  æ”¹è¿›ç‰ˆæœ¬: v1.1")
    print()

    all_reports = {}
    all_comparisons = {}

    for task_name, task_config in TASKS.items():
        model, report = run_task(task_name, task_config)
        all_reports[task_name] = report
        if report.get("vs_v1.0"):
            all_comparisons[task_name] = report["vs_v1.0"]

    # ä¿å­˜æ±‡æ€»
    summary_path = OUTPUT_DIR / "summary_v1.1.json"
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(all_reports, f, ensure_ascii=False, indent=2, default=str)

    comparison_path = OUTPUT_DIR / "comparison_v1_vs_v1.1.json"
    with open(comparison_path, "w", encoding="utf-8") as f:
        json.dump(all_comparisons, f, ensure_ascii=False, indent=2, default=str)

    print(f"\n\n{'='*70}")
    print(f"  âœ… v1.1 å…¨éƒ¨å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"  åŒ…å«æ–‡ä»¶:")
    for p in sorted(OUTPUT_DIR.iterdir()):
        size_kb = p.stat().st_size / 1024
        print(f"    {p.name:55s} ({size_kb:.1f} KB)")

    # æœ€ç»ˆæ€»ç»“
    print(f"\n{'='*70}")
    print(f"  ğŸ“Š v1.0 â†’ v1.1 æ”¹è¿›æ€»ç»“")
    print(f"{'='*70}")
    for task_name, comp in all_comparisons.items():
        print(f"\n  {task_name}:")
        print(f"    éªŒè¯é›† AUC: {comp['éªŒè¯é›†_çº¢å…‰']['AUC_å˜åŒ–']}")
        print(f"    æµ‹è¯•é›† AUC: {comp['æµ‹è¯•é›†_å–€å·¦']['AUC_å˜åŒ–']}")
        print(f"    æµ‹è¯•é›† F1:  {comp['æµ‹è¯•é›†_å–€å·¦']['F1_å˜åŒ–']}")


if __name__ == "__main__":
    main()

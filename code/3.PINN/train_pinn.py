"""
PINN è®­ç»ƒä¸»è„šæœ¬

æµç¨‹:
  1. åŠ è½½çº¢å…‰æ•°æ®ï¼ˆæ—¥å‡èšåˆï¼‰
  2. æ„å»ºè®­ç»ƒ/éªŒè¯é›†
  3. è®­ç»ƒ PINN (Data Loss + Physics Loss + Boundary Loss)
  4. åŒæ—¶å­¦ä¹ ç½‘ç»œæƒé‡å’Œç‰©ç†å‚æ•°
  5. è¯„ä¼° + å¯è§†åŒ– + åäº‹å®æ¨ç†
"""
import sys
import json
import warnings
from pathlib import Path
from datetime import datetime

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

warnings.filterwarnings("ignore")

from config import (
    DATA_DIR, OUTPUT_DIR, INPUT_COLS, TARGET_COL, AUX_COLS,
    PHYSICS_PARAMS, PARAM_BOUNDS, NET_CONFIG, TRAIN_CONFIG, RANDOM_SEED,
)
from physics import (
    PhysicsParams, do_saturation,
    compute_ode_residual, compute_boundary_loss,
)
from model import PINN


# ============================================================
# ä¸­æ–‡å­—ä½“
# ============================================================
def setup_chinese_font():
    for fp in ["/System/Library/Fonts/STHeiti Light.ttc",
               "/System/Library/Fonts/PingFang.ttc",
               "/System/Library/Fonts/Supplemental/Songti.ttc"]:
        try:
            fm.fontManager.addfont(fp)
            prop = fm.FontProperties(fname=fp)
            plt.rcParams["font.family"] = prop.get_name()
            plt.rcParams["axes.unicode_minus"] = False
            return
        except Exception:
            continue
    plt.rcParams["font.sans-serif"] = ["Arial Unicode MS", "SimHei"]
    plt.rcParams["axes.unicode_minus"] = False

setup_chinese_font()
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)


# ============================================================
# æ•°æ®åŠ è½½
# ============================================================
def load_data(site: str = "çº¢å…‰"):
    """åŠ è½½å¹¶èšåˆæ•°æ®ï¼Œè¿”å›æœ‰æ•ˆçš„æ—¥å‡ DataFrame"""
    path = DATA_DIR / f"cleaned_{site}.csv"
    df = pd.read_csv(path, parse_dates=["æ—¥æœŸ"])
    
    # æ—¥å‡èšåˆ
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    daily = df.groupby("æ—¥æœŸ")[numeric_cols].mean().sort_index()
    daily = daily.reset_index()
    
    # ç¡®å®šå¯ç”¨ç‰¹å¾åˆ—
    avail_input = [c for c in INPUT_COLS if c in daily.columns]
    avail_aux = [c for c in AUX_COLS if c in daily.columns]
    all_input = avail_input + avail_aux
    
    # ä»…ä¿ç•™ç›®æ ‡åˆ—éç©ºçš„è¡Œ
    mask = daily[TARGET_COL].notna()
    for c in avail_input:
        mask &= daily[c].notna()
    
    daily_clean = daily[mask].copy().reset_index(drop=True)
    
    # æ·»åŠ å½’ä¸€åŒ–æ—¶é—´åˆ— (0~1)
    dates = pd.to_datetime(daily_clean["æ—¥æœŸ"])
    t_days = (dates - dates.min()).dt.days.values.astype(float)
    t_norm = t_days / max(t_days.max(), 1.0)
    daily_clean["t_norm"] = t_norm
    daily_clean["t_days"] = t_days
    
    print(f"ğŸ“Š [{site}] æœ‰æ•ˆæ•°æ®: {len(daily_clean)} å¤©")
    print(f"   è¾“å…¥ç‰¹å¾: {all_input}")
    print(f"   ç›®æ ‡: {TARGET_COL}")
    print(f"   æ—¥æœŸ: {dates.min().date()} ~ {dates.max().date()}")
    
    return daily_clean, all_input


def prepare_tensors(df, input_cols, device):
    """å°† DataFrame è½¬ä¸º PyTorch å¼ é‡"""
    # ç‰¹å¾: [t_norm, input_cols...]
    feat_cols = ["t_norm"] + input_cols
    X = torch.tensor(df[feat_cols].values, dtype=torch.float32, device=device)
    y = torch.tensor(df[TARGET_COL].values, dtype=torch.float32, device=device)
    t = torch.tensor(df["t_norm"].values, dtype=torch.float32, device=device)
    t.requires_grad_(True)
    
    # ç‰¹å¾å½’ä¸€åŒ–ç»Ÿè®¡
    X_mean = X.mean(dim=0)
    X_std = X.std(dim=0).clamp(min=1e-6)
    
    return X, y, t, X_mean, X_std


# ============================================================
# è®­ç»ƒ
# ============================================================
def train_pinn(site: str = "çº¢å…‰"):
    """å®Œæ•´è®­ç»ƒæµç¨‹"""
    cfg = TRAIN_CONFIG
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"\n{'='*60}")
    print(f"  ğŸ§¬ PINN è®­ç»ƒ â€” æº¶æ°§åŠ¨åŠ›å­¦ ({site})")
    print(f"{'='*60}")
    print(f"  è®¾å¤‡: {device}")
    
    # 1. åŠ è½½æ•°æ®
    df, input_cols = load_data(site)
    n_total = len(df)
    
    # 2. åˆ’åˆ†è®­ç»ƒ/éªŒè¯ï¼ˆæ—¶é—´é¡ºåºï¼Œå 20% ä¸ºéªŒè¯ï¼‰
    n_val = int(n_total * cfg["val_ratio"])
    n_train = n_total - n_val
    df_train = df.iloc[:n_train].copy()
    df_val = df.iloc[n_train:].copy()
    print(f"  è®­ç»ƒ: {n_train} å¤©, éªŒè¯: {n_val} å¤©")
    
    # 3. å‡†å¤‡å¼ é‡
    X_train, y_train, t_train, X_mean, X_std = prepare_tensors(df_train, input_cols, device)
    X_val, y_val, t_val, _, _ = prepare_tensors(df_val, input_cols, device)
    
    # å½’ä¸€åŒ–
    X_train_norm = (X_train - X_mean) / X_std
    X_val_norm = (X_val - X_mean) / X_std
    
    # æå–æ°´æ¸©å’Œå…‰ç…§åˆ—ç´¢å¼•
    feat_cols = ["t_norm"] + input_cols
    idx_temp = feat_cols.index("æ°´æ¸©_æ—¥å‡")
    idx_light = feat_cols.index("å…‰ç…§æ—¶é•¿h") if "å…‰ç…§æ—¶é•¿h" in feat_cols else None
    
    # 4. åˆå§‹åŒ–æ¨¡å‹å’Œç‰©ç†å‚æ•°
    n_features = X_train_norm.shape[1]
    net = PINN(n_features, NET_CONFIG["hidden_layers"], NET_CONFIG["activation"]).to(device)
    physics = PhysicsParams(PHYSICS_PARAMS, PARAM_BOUNDS).to(device)
    
    print(f"  ç½‘ç»œå‚æ•°: {sum(p.numel() for p in net.parameters()):,}")
    print(f"  ç‰©ç†å‚æ•°: {sum(p.numel() for p in physics.parameters())}")
    
    # 5. ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam([
        {"params": net.parameters(), "lr": cfg["lr_net"]},
        {"params": physics.parameters(), "lr": cfg["lr_physics"]},
    ])
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=cfg["scheduler_step"], gamma=cfg["scheduler_gamma"]
    )
    
    # 6. è®­ç»ƒå¾ªç¯
    history = {"epoch": [], "loss": [], "loss_data": [], "loss_physics": [],
               "loss_boundary": [], "val_mae": []}
    best_val_mae = float("inf")
    best_state = None
    patience_counter = 0
    
    lambda_phys = cfg["lambda_physics"]
    lambda_bnd = cfg["lambda_boundary"]
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ ({cfg['epochs']} epochs)...")
    print(f"   Î»_physics={lambda_phys}, Î»_boundary={lambda_bnd}")
    
    for epoch in range(1, cfg["epochs"] + 1):
        net.train()
        physics.train()
        
        # --- æ„é€ éœ€è¦æ¢¯åº¦çš„è¾“å…¥ ---
        # t_input éœ€è¦ requires_grad=True ä»¥æ”¯æŒè‡ªåŠ¨å¾®åˆ† dDO/dt
        # å½’ä¸€åŒ– t: ä½¿ç”¨è®­ç»ƒé›†çš„ mean/std
        t_raw = X_train[:, 0:1].detach().clone()   # æœªå½’ä¸€åŒ–çš„ t_norm
        t_input = ((t_raw - X_mean[0]) / X_std[0]).requires_grad_(True)
        
        # å…¶ä»–ç‰¹å¾ä½¿ç”¨é¢„å½’ä¸€åŒ–çš„å€¼ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
        other_norm = X_train_norm[:, 1:].detach()
        X_input = torch.cat([t_input, other_norm], dim=1)
        
        # å‰å‘
        DO_pred = net(X_input)
        
        # Data Loss
        loss_data = nn.functional.mse_loss(DO_pred, y_train)
        
        # Physics Loss (ODE æ®‹å·®)
        T_water = X_train[:, idx_temp]  # æœªå½’ä¸€åŒ–çš„æ°´æ¸©
        light = X_train[:, idx_light] if idx_light is not None else torch.zeros_like(T_water)
        
        residual = compute_ode_residual(DO_pred, t_input, T_water, light, physics)
        loss_physics = torch.mean(residual ** 2)
        
        # Boundary Loss
        loss_boundary = compute_boundary_loss(DO_pred, T_water)
        
        # Total Loss
        loss = loss_data + lambda_phys * loss_physics + lambda_bnd * loss_boundary
        
        optimizer.zero_grad()
        loss.backward()
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        
        # Validation
        net.eval()
        with torch.no_grad():
            DO_val_pred = net(X_val_norm)
            val_mae = torch.mean(torch.abs(DO_val_pred - y_val)).item()
        
        # è®°å½•
        history["epoch"].append(epoch)
        history["loss"].append(loss.item())
        history["loss_data"].append(loss_data.item())
        history["loss_physics"].append(loss_physics.item())
        history["loss_boundary"].append(loss_boundary.item())
        history["val_mae"].append(val_mae)
        
        # Early stopping
        if val_mae < best_val_mae:
            best_val_mae = val_mae
            best_state = {
                "net": {k: v.clone() for k, v in net.state_dict().items()},
                "physics": {k: v.clone() for k, v in physics.state_dict().items()},
            }
            patience_counter = 0
        else:
            patience_counter += 1
        
        if epoch % cfg["print_every"] == 0 or epoch == 1:
            phys_vals = physics.get_all()
            print(f"  Epoch {epoch:4d} | Loss={loss.item():.4f} "
                  f"(D={loss_data.item():.4f} P={loss_physics.item():.4f} B={loss_boundary.item():.4f}) "
                  f"| Val MAE={val_mae:.4f} | K_La={phys_vals.get('K_La',0):.3f} "
                  f"R_fish={phys_vals.get('R_fish_base',0):.3f}")
        
        if patience_counter >= cfg["patience"]:
            print(f"\n  â¹ Early stopping at epoch {epoch} (patience={cfg['patience']})")
            break
    
    # æ¢å¤æœ€ä½³æ¨¡å‹
    if best_state is not None:
        net.load_state_dict(best_state["net"])
        physics.load_state_dict(best_state["physics"])
    
    print(f"\n  âœ… è®­ç»ƒå®Œæˆ! Best Val MAE = {best_val_mae:.4f} mg/L")
    print(f"  å­¦åˆ°çš„ç‰©ç†å‚æ•°:")
    learned_params = physics.get_all()
    for k, v in learned_params.items():
        print(f"    {k} = {v:.4f}")
    
    return net, physics, df, input_cols, X_mean, X_std, history, learned_params


# ============================================================
# å¯è§†åŒ–
# ============================================================
def plot_training_curves(history, site):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    epochs = history["epoch"]
    
    # Loss curves
    axes[0].plot(epochs, history["loss"], "b-", alpha=0.5, label="Total")
    axes[0].plot(epochs, history["loss_data"], "r-", alpha=0.7, label="Data")
    axes[0].plot(epochs, history["loss_physics"], "g-", alpha=0.7, label="Physics")
    axes[0].set_xlabel("Epoch")
    axes[0].set_ylabel("Loss")
    axes[0].set_title("è®­ç»ƒæŸå¤±")
    axes[0].legend()
    axes[0].set_yscale("log")
    axes[0].grid(True, alpha=0.3)
    
    # Val MAE
    axes[1].plot(epochs, history["val_mae"], "purple")
    axes[1].set_xlabel("Epoch")
    axes[1].set_ylabel("MAE (mg/L)")
    axes[1].set_title(f"éªŒè¯é›† MAE (Best={min(history['val_mae']):.4f})")
    axes[1].grid(True, alpha=0.3)
    
    # Physics loss ratio
    ratio = [p / (d + 1e-8) for p, d in zip(history["loss_physics"], history["loss_data"])]
    axes[2].plot(epochs, ratio, "orange")
    axes[2].set_xlabel("Epoch")
    axes[2].set_ylabel("Physics/Data æ¯”å€¼")
    axes[2].set_title("ç‰©ç†æŸå¤± / æ•°æ®æŸå¤±")
    axes[2].grid(True, alpha=0.3)
    
    fig.suptitle(f"PINN è®­ç»ƒè¿‡ç¨‹ â€” {site}", fontsize=14, fontweight="bold")
    plt.tight_layout()
    path = OUTPUT_DIR / f"training_curves_{site}.png"
    plt.savefig(path, dpi=150, bbox_inches="tight")
    plt.close()
    return path


def plot_predictions(net, physics, df, input_cols, X_mean, X_std, site, device):
    """ç»˜åˆ¶é¢„æµ‹ vs çœŸå®å¯¹æ¯”å›¾"""
    feat_cols = ["t_norm"] + input_cols
    X = torch.tensor(df[feat_cols].values, dtype=torch.float32, device=device)
    y = df[TARGET_COL].values
    
    X_norm = (X - X_mean) / X_std
    
    net.eval()
    with torch.no_grad():
        y_pred = net(X_norm).cpu().numpy()
    
    # DO é¥±å’Œåº¦
    T_water = df["æ°´æ¸©_æ—¥å‡"].values
    DO_sat = 14.62 - 0.3898 * T_water + 0.006969 * T_water**2 - 5.897e-5 * T_water**3
    
    dates = pd.to_datetime(df["æ—¥æœŸ"])
    
    fig, axes = plt.subplots(2, 1, figsize=(16, 10))
    
    # ä¸Šå›¾ï¼šé¢„æµ‹ vs çœŸå®
    axes[0].plot(dates, y, "b-", alpha=0.6, linewidth=1, label="å®æµ‹ DO")
    axes[0].plot(dates, y_pred, "r-", alpha=0.8, linewidth=1.5, label="PINN é¢„æµ‹")
    axes[0].plot(dates, DO_sat, "g--", alpha=0.4, linewidth=1, label="é¥±å’Œ DO")
    axes[0].fill_between(dates, 0, 2, color="red", alpha=0.1, label="å±é™©åŒº (<2 mg/L)")
    axes[0].set_ylabel("æº¶æ°§ (mg/L)")
    axes[0].set_title(f"PINN æº¶æ°§é¢„æµ‹ â€” {site}")
    axes[0].legend(loc="upper right")
    axes[0].grid(True, alpha=0.3)
    
    # ä¸‹å›¾ï¼šæ®‹å·®
    residual = y - y_pred
    axes[1].bar(dates, residual, color="steelblue", alpha=0.6, width=1)
    axes[1].axhline(y=0, color="black", linewidth=0.5)
    mae = np.mean(np.abs(residual))
    rmse = np.sqrt(np.mean(residual**2))
    axes[1].set_ylabel("æ®‹å·® (mg/L)")
    axes[1].set_title(f"é¢„æµ‹æ®‹å·® | MAE={mae:.4f} RMSE={rmse:.4f}")
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    path = OUTPUT_DIR / f"prediction_{site}.png"
    plt.savefig(path, dpi=150, bbox_inches="tight")
    plt.close()
    
    return path, mae, rmse


def plot_counterfactual(net, physics, df, input_cols, X_mean, X_std, site, device):
    """åäº‹å®æ¨ç†ï¼šæ¸©åº¦å‡é«˜ / åœæ­¢æ›æ°” / é±¼å¯†åº¦ç¿»å€"""
    feat_cols = ["t_norm"] + input_cols
    X_base = torch.tensor(df[feat_cols].values, dtype=torch.float32, device=device)
    X_base_norm = (X_base - X_mean) / X_std
    
    idx_temp = feat_cols.index("æ°´æ¸©_æ—¥å‡")
    dates = pd.to_datetime(df["æ—¥æœŸ"])
    
    net.eval()
    
    scenarios = {}
    
    # åŸºçº¿
    with torch.no_grad():
        y_base = net(X_base_norm).cpu().numpy()
    scenarios["åŸºçº¿ (å½“å‰)"] = y_base
    
    # åœºæ™¯1: æ°´æ¸©å‡é«˜ 3â„ƒ
    X_warm = X_base.clone()
    X_warm[:, idx_temp] += 3.0
    X_warm_norm = (X_warm - X_mean) / X_std
    with torch.no_grad():
        y_warm = net(X_warm_norm).cpu().numpy()
    scenarios["æ°´æ¸© +3â„ƒ"] = y_warm
    
    # åœºæ™¯2: æ°´æ¸©é™ä½ 3â„ƒ
    X_cool = X_base.clone()
    X_cool[:, idx_temp] -= 3.0
    X_cool_norm = (X_cool - X_mean) / X_std
    with torch.no_grad():
        y_cool = net(X_cool_norm).cpu().numpy()
    scenarios["æ°´æ¸© -3â„ƒ"] = y_cool
    
    # ç»˜å›¾
    fig, ax = plt.subplots(figsize=(16, 6))
    colors = {"åŸºçº¿ (å½“å‰)": "blue", "æ°´æ¸© +3â„ƒ": "red", "æ°´æ¸© -3â„ƒ": "green"}
    for name, y_vals in scenarios.items():
        ax.plot(dates, y_vals, color=colors[name], alpha=0.7, linewidth=1.5, label=name)
    
    ax.fill_between(dates, 0, 2, color="red", alpha=0.1, label="å±é™©åŒº (<2 mg/L)")
    ax.set_xlabel("æ—¥æœŸ")
    ax.set_ylabel("æº¶æ°§ (mg/L)")
    ax.set_title(f"åäº‹å®æ¨ç† â€” {site} | æ°´æ¸©å˜åŒ–å¯¹æº¶æ°§çš„å½±å“")
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    path = OUTPUT_DIR / f"counterfactual_{site}.png"
    plt.savefig(path, dpi=150, bbox_inches="tight")
    plt.close()
    
    return path, scenarios


# ============================================================
# ä¸»æµç¨‹
# ============================================================
def main():
    print("=" * 60)
    print("  ğŸ§¬ PINN â€” æº¶æ°§åŠ¨åŠ›å­¦æ¨¡å‹")
    print("=" * 60)
    print(f"  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  è¾“å‡º: {OUTPUT_DIR}")
    
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    
    all_results = {}
    
    for site in ["çº¢å…‰", "å–€å·¦"]:
        print(f"\n\n{'='*60}")
        print(f"  ğŸ“ ç«™ç‚¹: {site}")
        print(f"{'='*60}")
        
        # è®­ç»ƒ
        net, physics, df, input_cols, X_mean, X_std, history, learned_params = train_pinn(site)
        
        # è®­ç»ƒæ›²çº¿
        curve_path = plot_training_curves(history, site)
        print(f"\n  ğŸ“Š è®­ç»ƒæ›²çº¿: {curve_path}")
        
        # é¢„æµ‹å›¾
        pred_path, mae, rmse = plot_predictions(
            net, physics, df, input_cols, X_mean, X_std, site, device
        )
        print(f"  ğŸ“Š é¢„æµ‹å›¾: {pred_path}")
        print(f"  MAE={mae:.4f}, RMSE={rmse:.4f}")
        
        # åäº‹å®æ¨ç†
        cf_path, cf_scenarios = plot_counterfactual(
            net, physics, df, input_cols, X_mean, X_std, site, device
        )
        print(f"  ğŸ“Š åäº‹å®æ¨ç†: {cf_path}")
        
        # ä¿å­˜æ¨¡å‹
        model_path = OUTPUT_DIR / f"pinn_{site}.pt"
        torch.save({
            "net_state": net.state_dict(),
            "physics_state": physics.state_dict(),
            "X_mean": X_mean.cpu(),
            "X_std": X_std.cpu(),
            "input_cols": input_cols,
        }, model_path)
        print(f"  ğŸ’¾ æ¨¡å‹: {model_path}")
        
        # æ±‡æ€»
        all_results[site] = {
            "MAE": float(mae),
            "RMSE": float(rmse),
            "best_val_mae": float(min(history["val_mae"])),
            "learned_params": learned_params,
            "n_data": len(df),
        }
    
    # ä¿å­˜æŠ¥å‘Š
    report = {
        "æ¨¡å‹": "PINN (æº¶æ°§åŠ¨åŠ›å­¦)",
        "æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "ç‰©ç†æ–¹ç¨‹": "dDO/dt = K_La*(DO_sat(T)-DO) - R_fish(T) - R_bio + P_photo",
        "ç»“æœ": all_results,
    }
    report_path = OUTPUT_DIR / "report.json"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\n\n{'='*60}")
    print(f"  âœ… PINN è®­ç»ƒå…¨éƒ¨å®Œæˆ!")
    print(f"{'='*60}")
    for p in sorted(OUTPUT_DIR.iterdir()):
        print(f"    {p.name:40s} ({p.stat().st_size/1024:.1f} KB)")


if __name__ == "__main__":
    main()

"""
æ­¥éª¤6ï¼šå¯¼å‡ºæ¸…æ´—åæ•°æ® + è´¨é‡æŠ¥å‘Š
"""

import pandas as pd
import numpy as np
import os


OUTPUT_DIR = os.path.join(os.path.dirname(__file__), '..', '..', 'data')

# æ•°å€¼èŒƒå›´åˆç†æ€§æ£€æŸ¥è§„åˆ™
RANGE_CHECKS = {
    'æ°´æ¸©_æ—¥å‡': (0, 45),
    'æœ€ä½æ°´æ¸©â„ƒ': (0, 45),
    'æœ€é«˜æ°´æ¸©â„ƒ': (0, 45),
    'æ°”æ¸©_æ—¥å‡': (-30, 55),
    'æœ€ä½æ°”æ¸©â„ƒ': (-30, 55),
    'æœ€é«˜æ°”æ¸©â„ƒ': (-30, 55),
    'æ¹¿åº¦_æ—¥å‡': (0, 100),
    'æœ€ä½æ¹¿åº¦%': (0, 100),
    'æœ€é«˜æ¹¿åº¦%': (0, 100),
    'æº¶æ°§mg/L': (0, 25),
    'æ°¨æ°®mg/L': (0, 50),
    'PH': (3, 11),
    'ECå€¼ms/cm': (0, 20),
    'å…‰ç…§æ—¶é•¿h': (0, 24),
}


def validate_and_export(data: dict):
    """éªŒè¯æ•°æ®è´¨é‡å¹¶å¯¼å‡º"""
    print('\nğŸ“¦ æ­¥éª¤ 6ï¼šéªŒè¯ä¸å¯¼å‡º')
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    for base in ['çº¢å…‰', 'å–€å·¦']:
        key = f'{base}_åˆå¹¶'
        if key not in data:
            continue

        df = data[key]
        print(f'\n  ğŸ“ {base}åŸºåœ° â€” æœ€ç»ˆæ•°æ®: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—')

        # â”€â”€ 1. ä¸»é”®å®Œæ•´æ€§ â”€â”€
        print('\n  ğŸ” ä¸»é”®å®Œæ•´æ€§:')
        for col in ['æ—¥æœŸ', 'åŸºåœ°', 'æ¨¡å—']:
            if col in df.columns:
                n_null = df[col].isna().sum()
                status = 'âœ…' if n_null == 0 else 'âŒ'
                print(f'    {status} {col}: {n_null} ä¸ªç©ºå€¼')

        # â”€â”€ 2. ä¸»é”®å”¯ä¸€æ€§ â”€â”€
        dup_count = df.duplicated(subset=['æ—¥æœŸ', 'æ¨¡å—']).sum()
        status = 'âœ…' if dup_count == 0 else 'âŒ'
        print(f'    {status} (æ—¥æœŸ, æ¨¡å—) é‡å¤è¡Œ: {dup_count}')

        # â”€â”€ 3. ç¼ºå¤±ç‡ç»Ÿè®¡ â”€â”€
        print('\n  ğŸ“Š å„åˆ—ç¼ºå¤±ç‡:')
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            miss = df[col].isna().sum()
            rate = miss / len(df) * 100
            if miss > 0:
                bar = 'â–ˆ' * int(rate / 5) + 'â–‘' * (20 - int(rate / 5))
                print(f'    {col:30s}: {miss:>6d} ({rate:5.1f}%) {bar}')
        zero_miss = [c for c in numeric_cols if df[c].isna().sum() == 0]
        print(f'    âœ… {len(zero_miss)} åˆ—é›¶ç¼ºå¤±: {zero_miss[:5]}{"..." if len(zero_miss) > 5 else ""}')

        # â”€â”€ 4. æ•°å€¼èŒƒå›´æ£€æŸ¥ â”€â”€
        print('\n  ğŸ” æ•°å€¼èŒƒå›´æ£€æŸ¥:')
        for col, (lo, hi) in RANGE_CHECKS.items():
            if col not in df.columns:
                continue
            valid = df[col].dropna()
            if len(valid) == 0:
                continue
            out_of_range = ((valid < lo) | (valid > hi)).sum()
            status = 'âœ…' if out_of_range == 0 else 'âš ï¸'
            print(f'    {status} {col}: [{valid.min():.2f}, {valid.max():.2f}]'
                  f' (æœŸæœ› [{lo}, {hi}], è¶Šç•Œ {out_of_range} æ¡)')

        # â”€â”€ 5. æ—¶é—´è¿ç»­æ€§æ£€æŸ¥ â”€â”€
        print('\n  ğŸ” æ—¶é—´è¿ç»­æ€§ (æŠ½æŸ¥å‰ 3 ä¸ªæ¨¡å—):')
        modules = df['æ¨¡å—'].unique()[:3]
        for mod in modules:
            mod_df = df[df['æ¨¡å—'] == mod].sort_values('æ—¥æœŸ')
            dates = mod_df['æ—¥æœŸ'].dt.date
            if len(dates) < 2:
                continue
            gaps = pd.Series(dates.values[1:]) - pd.Series(dates.values[:-1])
            max_gap = gaps.max()
            n_gaps = (gaps > pd.Timedelta(days=1)).sum()
            print(f'    {mod}: {len(dates)} å¤©, æœ€å¤§é—´éš” {max_gap.days}å¤©, é—´æ–­ {n_gaps} å¤„')

        # â”€â”€ 6. å¯¼å‡º CSV â”€â”€
        out_path = os.path.join(OUTPUT_DIR, f'merged_{base}.csv')
        df.to_csv(out_path, index=False, encoding='utf-8-sig')
        size_mb = os.path.getsize(out_path) / 1024 / 1024
        print(f'\n  ğŸ’¾ å·²å¯¼å‡º: {out_path} ({size_mb:.1f} MB)')

        # â”€â”€ 7. åˆ—æ¸…å• â”€â”€
        print(f'\n  ğŸ“‹ æœ€ç»ˆåˆ—æ¸…å• ({len(df.columns)} åˆ—):')
        for i, col in enumerate(df.columns):
            dtype = df[col].dtype
            print(f'    {i+1:>3d}. {col} ({dtype})')


if __name__ == '__main__':
    from _01_load_and_fix import load_all
    from _02_parse_hourly import parse_all_hourly
    from _03_parse_disease import parse_all_disease
    from _04_time_align_merge import time_align_merge
    from _05_imputation import impute_all
    data = load_all()
    data = parse_all_hourly(data)
    data = parse_all_disease(data)
    data = time_align_merge(data)
    data = impute_all(data)
    validate_and_export(data)
